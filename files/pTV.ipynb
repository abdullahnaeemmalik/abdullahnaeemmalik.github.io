{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1520285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from time import process_time\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "from torch.sparse import *\n",
    "import collections\n",
    "os.environ['DGLBACKEND'] = 'pytorch'\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ogb.nodeproppred import DglNodePropPredDataset, Evaluator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0929d",
   "metadata": {},
   "source": [
    "This builds the 1-skeleton of a standard simplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d6fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplexCreator():\n",
    "    \"\"\"Create standard simplex\"\"\"\n",
    "    def __init__(self, dimension):\n",
    "        self.input_dimension = dimension\n",
    "        self.src=list()\n",
    "        self.dst=list()\n",
    "        for i in range(self.input_dimension+1):\n",
    "            for j in range(self.input_dimension+1):\n",
    "                if (i < j):\n",
    "                    self.src = self.src + [i]\n",
    "                    self.dst = self.dst + [j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a745612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class psuedotv():\n",
    "    src=list()\n",
    "    dst=list()\n",
    "    empty_graph = dgl.heterograph({('node', 'to', 'node'): (src, dst)})\n",
    "    dimension = int\n",
    "\n",
    "    assert isinstance(empty_graph, dgl.DGLHeteroGraph), \\\n",
    "        'Keyword argument \\\"graph\\\" of AdjGraph\\'s init methodmust be a dgl.DGLHeteroGraph.'\n",
    "\n",
    "    def __init__(\n",
    "        self, file_path, graph=empty_graph,dimension=dimension):\n",
    "        self.seed_graph = graph\n",
    "        self.srcs_and_dsts = self.seed_graph.edges()   \n",
    "        \n",
    "        \"\"\"Create dictionary with dimension (keys) and place list of nodes with all possible top dimensions (values)\"\"\"\n",
    "        self.top_vertices_dictionary = collections.defaultdict(list)\n",
    "        self.top_vertices_dictionary[0]=[int(x) for x in self.seed_graph.nodes()]\n",
    "        self.top_vertices_dictionary[1]=[int(x) for x in torch.unique(self.srcs_and_dsts[1])]\n",
    "        \"\"\" This dict has keys = dimensions d and values = dictionary. This subdictionary has keys = nodes and values =\n",
    "        vertices that make the node a top-vertex of dimension d\"\"\"\n",
    "\n",
    "        print(\"Finished adding 0-top vertices and 1-top vertices in main dictionary\")\n",
    "        \n",
    "        \"\"\"compute all in_degrees. These are needed for the algorithm later on when the loop runs\"\"\"\n",
    "        self.in_degrees = self.seed_graph.in_degrees()\n",
    "        self.out_degrees = self.seed_graph.out_degrees()\n",
    "        self.dimension = dimension\n",
    "        self.maximum_dimension = int(torch.max(self.in_degrees))\n",
    "        \n",
    "        self.file_path=file_path\n",
    "        \n",
    "        \"\"\"Create dictionary with dimension (keys) and list of nodes with maximum\n",
    "        dimension corresponding to key (values)\"\"\"\n",
    "        self.pseudo_top_vertices_dict = collections.defaultdict(list)\n",
    "        for v in self.seed_graph.nodes():\n",
    "            \"\"\"Add all zero-dimension vertices, and for now keep rest as 1-dimension vertices.\n",
    "            The vertices moved to different keys as more simplices are identified\"\"\"\n",
    "            if self.in_degrees[v] == 0:\n",
    "                self.pseudo_top_vertices_dict[0] = self.pseudo_top_vertices_dict[0] + [int(v)]\n",
    "            else:\n",
    "                self.pseudo_top_vertices_dict[1] = self.pseudo_top_vertices_dict[1] + [int(v)]\n",
    "                \n",
    "        print(\"Finished adding 0-pseudo top vertices and 1-pseudo top vertices in main dictionary\")\n",
    "        \n",
    "        \"\"\"Create empty dictionary as above, but this time will have sets (value) for each dimension (key)\"\"\"        \n",
    "        self.pseudo_top_vertices_dict_of_sets = dict()\n",
    "        \n",
    "        \"\"\"Same values as above, but keys are addresses of the sets\"\"\" \n",
    "        self._sets = dict()\n",
    "        \n",
    "        \"\"\" Creates dictionary from above sets that has representatives (keys) and sets (values)\"\"\"\n",
    "        self._partition = dict()\n",
    "\n",
    "        \"\"\" Needed for refinement. Finds vertices in same class for each iteration of refinement\"\"\"\n",
    "        self.refined_partition = dict()\n",
    "        \n",
    "        \"\"\" Number of refinements done after TV identification.for each vertex. Needed to find final partition index\"\"\"\n",
    "        self.partition_number = 0\n",
    "        \n",
    "        \"\"\" Dictionary with keys = nodes as values = partition index of that node. Currently at zero, since\n",
    "        no refinement is done, yet! The values for each key is supposed to be the partition_number\"\"\"\n",
    "        self.partition_index = {int_node: 0 for int_node in self.top_vertices_dictionary[0]}\n",
    "        \n",
    "        \"\"\"Create dictionary with keys = nodes and values = one-hot tensor of top-vertex and parition index,\n",
    "        both concatenated\"\"\"\n",
    "        self.partition_indices_and_one_hot_tv = {int_node: [] for int_node in self.top_vertices_dictionary[0]}\n",
    "        \n",
    "        \"\"\" Create dictionary with keys = nodes and values = one hot encoding of pseudo top dimension. That is\n",
    "        ith index 1 if top maximum dimension is equal to index and zero otherwise \"\"\"\n",
    "        self.one_hot_dict = collections.defaultdict(list)\n",
    "\n",
    "        \"\"\" Create dictionary with keys = nodes. The values for the dictionary \n",
    "        are tensors which are multi hot encoding with ith index 1 for i-top dimension and zero otherwise \"\"\"\n",
    "        self.multi_hot_tv_dict = collections.defaultdict(list)\n",
    "        \n",
    "        \"\"\" Create dictionary with keys = nodes and values = one hot encoding with ith index 1 if vertex is refined i\n",
    "        times and zero otherwise. This is an index of number of times a vertex has been partitioned. \"\"\"\n",
    "        self.partition_times_hot_dict = collections.defaultdict(list)\n",
    "        \n",
    "        \"\"\" Create dictionary with keys = nodes and values = one hot encoding with ith index 1 if vertex is in the\n",
    "        ith partition and zero otherwise. This captures the number of partitions after refinement. \"\"\"\n",
    "        self.partitioned_tv = collections.defaultdict(list)\n",
    "        \n",
    "        \"\"\" Create a dictionary of vectors R(v) for each vertex v. This gets updated at each refinement process. \n",
    "        The values are stored since then the algorithm wouldn't have to create the vector each time its needed\"\"\"\n",
    "        self.refinement_vectors_dict = collections.defaultdict(list)\n",
    "        \n",
    "        \"\"\" Boolean expression to see if refinement needs to proceed\"\"\"\n",
    "        self.partition_proceeds = True\n",
    "        print(\"Computing matrices of dimension 2\")\n",
    "        diagonal_mask = (self.seed_graph.adj_external()._indices()[0] == self.seed_graph.adj_external()._indices()[1])\n",
    "        off_diagonal_mask = ~diagonal_mask\n",
    "        self.seed_graph.adj_external()._values()[off_diagonal_mask] = 1.0\n",
    "        new_indices = self.seed_graph.adj_external()._indices()[:, off_diagonal_mask]\n",
    "        new_values = self.seed_graph.adj_external()._values()[off_diagonal_mask]\n",
    "        new_size = self.seed_graph.adj_external().size()\n",
    "        new_sparse_matrix = torch.sparse_coo_tensor(indices=new_indices, values=new_values, size=new_size)\n",
    "        self.hadamard_product_prev = new_sparse_matrix\n",
    "        self.adj_matrix_without_diag = new_sparse_matrix\n",
    "        self.powers_of_adj_matrix_witout_diag = new_sparse_matrix\n",
    "        self.hadamard_product_next = torch.mul(new_sparse_matrix,torch.sparse.mm(new_sparse_matrix,new_sparse_matrix))\n",
    "        print(\"Finished computing matrices for dimension 2\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the partition.\"\"\"\n",
    "        return len(self._sets)\n",
    "\n",
    "    def out_nodes_as_int(self, vertex):\n",
    "        \"\"\"convert successors to a list with integer node values\"\"\"\n",
    "        neighbors = [int(v) for v in list(self.seed_graph.successors(vertex))]\n",
    "        if int(vertex) in neighbors:\n",
    "            neighbors.remove(int(vertex))\n",
    "        return neighbors\n",
    "\n",
    "    def in_nodes_as_int(self, vertex):\n",
    "        \"\"\"convert predecessors to a list with integer node values\"\"\"\n",
    "        neighbors = [int(v) for v in list(self.seed_graph.predecessors(vertex))]\n",
    "        if int(vertex) in neighbors:\n",
    "            neighbors.remove(int(vertex))\n",
    "        return neighbors       \n",
    "    \n",
    "    def inductive_connecting(self):\n",
    "        #Case for k=2 doesn't need to search for intersections:\n",
    "        print(\"Adding 2-vertices to the dictionary\")\n",
    "        row_indices_analyzed_node, col_indices_analyzed_node = self.hadamard_product_next._indices()\n",
    "        for vertex in col_indices_analyzed_node.unique():\n",
    "            self.top_vertices_dictionary[2] = self.top_vertices_dictionary[2] + [int(vertex)]\n",
    "            self.pseudo_top_vertices_dict[2] = self.pseudo_top_vertices_dict[2] + [int(vertex)]\n",
    "            if vertex in self.pseudo_top_vertices_dict[1]:\n",
    "                self.pseudo_top_vertices_dict[1].remove(int(vertex))\n",
    "        print(\"Finished adding vertices of dimension 2\")    \n",
    "        \n",
    "        for k in tqdm(range(3,self.dimension+1), position=0, leave=False):\n",
    "            temp_power = torch.sparse.mm(self.powers_of_adj_matrix_witout_diag, self.adj_matrix_without_diag)\n",
    "            diagonal_mask = (temp_power._indices()[0] == temp_power._indices()[1])\n",
    "            off_diagonal_mask = ~diagonal_mask\n",
    "            temp_power._values()[off_diagonal_mask] = 1.0\n",
    "            new_indices = temp_power._indices()[:, off_diagonal_mask]\n",
    "            new_values = temp_power._values()[off_diagonal_mask]\n",
    "            new_size = temp_power.size()\n",
    "            new_sparse_matrix = torch.sparse_coo_tensor(indices=new_indices, values=new_values, size=new_size)\n",
    "            self.powers_of_adj_matrix_no_diag = new_sparse_matrix\n",
    "            #compute A⚬A^2⚬..⚬A^k where ⚬ denotes Hadamard product\n",
    "            self.hadamard_product_prev  = self.hadamard_product_next.clone().detach()\n",
    "            self.hadamard_product_next = torch.mul(self.hadamard_product_prev,self.powers_of_adj_matrix_no_diag)\n",
    "            print(\"Finished computing matrices for dim\",k)\n",
    "            \n",
    "            for v in tqdm(self.seed_graph.nodes(), position=0, leave=False):\n",
    "                if v in self.top_vertices_dictionary[k-1]:\n",
    "                    if self.in_degrees[v] < k:\n",
    "                        continue\n",
    "                    else:\n",
    "                        current_in_neighbors_of_node_analyzed = self.in_nodes_as_int(v)\n",
    "                        for u in tqdm(self.top_vertices_dictionary[k-1], position=0, leave=False):\n",
    "                            if u == v:\n",
    "                                continue\n",
    "                            if u not in current_in_neighbors_of_node_analyzed:\n",
    "                                continue\n",
    "                            else:\n",
    "                                A = self.hadamard_product_prev\n",
    "                                B = self.hadamard_product_next\n",
    "                                A = A.coalesce()\n",
    "                                B = B.coalesce()\n",
    "                                # Get the row and column indices of the nonzero elements in each matrix\n",
    "                                A_row, A_col = A.indices()\n",
    "                                B_row, B_col = B.indices()\n",
    "                                if len(B_col) == 0:\n",
    "                                    print(\"We have reached maximum dimension, and that is\",k-1)\n",
    "                                    self.maximum_dimension = k-1\n",
    "                                    self.top_vertices_dictionary.pop(k, None)\n",
    "                                    return\n",
    "                                # Find the rows that have a nonzero element in the u-th column of A\n",
    "                                A_u_rows = A_row[torch.where(A_col.eq(u))]\n",
    "                                # Find the rows that have a nonzero element in the v-th column of B\n",
    "                                B_v_rows = B_row[torch.where(B_col.eq(v))]\n",
    "                                # Find the common rows between A_u_rows and B_v_rows\n",
    "                                common_rows = np.intersect1d(A_u_rows, B_v_rows)\n",
    "                                #these are all the vertices for which there is a path with unique vertices\n",
    "                                #of length 1, length 2, ..., length k-1 to both u and v\n",
    "                                intersection_criterion = False\n",
    "                                for i in common_rows:\n",
    "                                    intersection = set(self.out_nodes_as_int(i)).intersection(\n",
    "                                        set(self.in_nodes_as_int(u))).intersection(set(current_in_neighbors_of_node_analyzed))\n",
    "                                    if len(intersection) > k-3:\n",
    "                                        intersection_criterion = True\n",
    "                                        break\n",
    "                                if not(intersection_criterion):\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    self.top_vertices_dictionary[k] = self.top_vertices_dictionary[k] + [int(v)]\n",
    "                                    self.pseudo_top_vertices_dict[k] = self.pseudo_top_vertices_dict[k] + [int(v)]\n",
    "                                    if v in self.pseudo_top_vertices_dict[k-1]:\n",
    "                                        self.pseudo_top_vertices_dict[k-1].remove(int(v))\n",
    "                                        \"\"\"a new top vertex v is found, so we can move out of the neighborhood of v\"\"\"\n",
    "                                    break\n",
    "                        \n",
    "            if len(self.top_vertices_dictionary[k]) == 0:\n",
    "                print(\"We have reached maximum dimension, and that is\",k-1)\n",
    "                self.maximum_dimension = k-1\n",
    "                self.top_vertices_dictionary.pop(k, None)\n",
    "                break\n",
    "        \n",
    "        print(\"Now creating other initial dictionaries\")\n",
    "        self.pseudo_top_vertices_dict_of_sets = {key:set(self.pseudo_top_vertices_dict[key]) \n",
    "                                                for key in range(0,self.maximum_dimension+1)}          \n",
    "        self._sets = {id(self.pseudo_top_vertices_dict_of_sets[key]):self.pseudo_top_vertices_dict_of_sets[key]\n",
    "                                 for key in self.pseudo_top_vertices_dict_of_sets.keys()}\n",
    "        self._partition = {x:self.pseudo_top_vertices_dict_of_sets[key] \n",
    "                              for key in self.pseudo_top_vertices_dict_of_sets.keys() \n",
    "                           for x in self.pseudo_top_vertices_dict_of_sets[key]}\n",
    "        print(\"Finished creating initial partition\")\n",
    "        print(\"Serializing dictionaries..\")\n",
    "        data_to_save = {\"pseudo_top_vertices_dict\": {str(key): value for key, value in self.pseudo_top_vertices_dict.items()}}\n",
    "        \n",
    "        with open('pseudo_tv.json', \"w\") as file:\n",
    "            json.dump(data_to_save, file)\n",
    "        print(\"Dictionary of pseudo top vertices saved as a JSON file\")\n",
    "    \n",
    "    def partition_vector(self,vertex):\n",
    "        \"\"\" Create vector R(v) for vertex v \"\"\" \n",
    "        vector = [self._partition[vertex]]\n",
    "        for v in torch.sort(self.seed_graph.predecessors(vertex))[0]:\n",
    "            \"\"\" The representatives have to be sorted for a meaningful comparison of vectors \"\"\"\n",
    "            if torch.eq(v,vertex):\n",
    "                pass\n",
    "            else:\n",
    "                vector = vector + [self._partition[int(v)]]\n",
    "        return vector\n",
    "        \n",
    "    def refine(self):\n",
    "        \"\"\"Original idea for refinement algorithm by David Eppstein. \n",
    "        Refine each set A in the partition to the two sets\n",
    "        A & S, A - S.  Also produces output (A & S, A - S)\n",
    "        for each changed set.  Within each pair, A & S will be\n",
    "        a newly created set, while A - S will be a modified\n",
    "        version of an existing set in the partition.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\" hit here is a dictionary with keys = addresses for original partitions and values = vertices with common\n",
    "        partition vector\"\"\"\n",
    "        hit = self.refined_partition\n",
    "        output = list()\n",
    "        for A,AS in hit.items():\n",
    "            A = self._sets[A]\n",
    "            \"\"\"Check if new partition is not the same as the old partition\"\"\"\n",
    "            if AS != A:\n",
    "                self._sets[id(AS)] = AS\n",
    "                \"\"\" This loop finds elements that are not part of previous partition\"\"\"\n",
    "                for x in AS:\n",
    "                    self._partition[x] = AS\n",
    "                \"\"\"The elements that were not part of the partition are now A\"\"\"\n",
    "                A -= AS\n",
    "                output = output + [set.union(A,AS)]\n",
    "        \"\"\" output here keeps track of those partitions that have been broken down\"\"\"\n",
    "        refined_vertices = set().union(*output)\n",
    "        \"\"\"The partitioning process above, once done, should then increase the partition_number, if the above\n",
    "        does indeed count as another genuine refinement. If it does not, then the number of refined_vertices\n",
    "        is zero, and hence should not increase partition number\"\"\"\n",
    "        if len(refined_vertices) == 0:\n",
    "            self.partition_proceeds = False\n",
    "            return\n",
    "        else:\n",
    "            \"\"\"If there is a refinement that takes place, then we increase the partition number and attach\n",
    "            this as the partition index for each vertex in the partition_index dictionary\"\"\"\n",
    "            self.partition_number = self.partition_number+1\n",
    "            for v in refined_vertices:\n",
    "                self.partition_index[v] = self.partition_number\n",
    "\n",
    "    def refinement(self):\n",
    "        \"\"\"Keep on refining the partitions until the partition stabilizes\"\"\"\n",
    "        while self.partition_proceeds:\n",
    "            print(\"Refining..\")\n",
    "            \"\"\"finds vertices u and v such that R(u) = R(v) and make refined partitions here\"\"\"\n",
    "            common_vertices = dict()\n",
    "            for node in tqdm(self.seed_graph.nodes(), position=0, leave=False):\n",
    "                self.refinement_vectors_dict[int(node)] = self.partition_vector(int(node))\n",
    "            print(\"Finished creating list of partition vectors for partition iteration number\",self.partition_number)\n",
    "            print(\"Finding vertices with common partition vectors..\")\n",
    "            \"\"\"This step could be modified for optimization. It needlessly also checks\n",
    "            for partion vectors of vertices that have not been partitioned the first time\"\"\"\n",
    "            for v,u in tqdm(itertools.combinations(self.seed_graph.nodes(),2), position=0, leave=False):\n",
    "                if self.refinement_vectors_dict[int(v)] == self.refinement_vectors_dict[int(u)]:\n",
    "                    Au = self._partition[int(u)]\n",
    "                    common_vertices.setdefault(id(Au),set()).update([int(u),int(v)])\n",
    "            self.refined_partition=common_vertices  \n",
    "            self.refine()\n",
    "        print(\"Refinement process finished\")\n",
    "\n",
    "    def add_vertex_features(self):\n",
    "        \"\"\"First, we start with refinement until the partition stabilizes\"\"\"\n",
    "        sets_for_partition_as_list = list(self._sets.values())\n",
    "        print(\"Adding in node features..\")\n",
    "        for node in tqdm(self.seed_graph.nodes(), position=0, leave=False):\n",
    "            \"\"\" Fills in the following dictionaries\n",
    "            1) multi_hot_tv_dict\n",
    "            2) one_hot_dict\n",
    "            3) partition_indices_and_one_hot_tv\n",
    "            4) partition_times_hot_dict\n",
    "            5) partitioned_tv\n",
    "            by filling in each key (node)\"\"\"\n",
    "            pihvector = [0] * (self.partition_number+1)\n",
    "            mhvector = [0] * (self.maximum_dimension+1)\n",
    "            ohvector = [0] * (self.maximum_dimension+1)\n",
    "            ptvvector = [0] * (len(self._sets))\n",
    "            node = int(node)\n",
    "            for dim in range(0,self.maximum_dimension+1):\n",
    "                if node in self.top_vertices_dictionary[dim]:\n",
    "                    mhvector[dim] = 1\n",
    "                if node in self.pseudo_top_vertices_dict[dim]:\n",
    "                    ohvector[dim] = 1\n",
    "            pihvector[self.partition_index[node]] = 1\n",
    "            self.multi_hot_tv_dict[node] = mhvector\n",
    "            self.one_hot_dict[node] = ohvector\n",
    "            self.partition_times_hot_dict[node] = pihvector\n",
    "            self.partition_indices_and_one_hot_tv[node] = self.one_hot_dict[node] + self.partition_times_hot_dict[node]\n",
    "            index = sets_for_partition_as_list.index(self._partition[node])\n",
    "            ptvvector[index] = 1\n",
    "            self.partitioned_tv[node] = ptvvector\n",
    "        print(\"Vertex features added!\")\n",
    "          \n",
    "    def save_dicts(self):\n",
    "        print(\"Serializing dictionaries for vertex features..\")\n",
    "        data_to_save = {\n",
    "            \"partitioned_tv\": {str(key): value for key, value in self.partitioned_tv.items()},\n",
    "            \"partition_indices_and_one_hot_tv\": {str(key): value for key, value in self.partition_indices_and_one_hot_tv.items()},\n",
    "            \"multi_hot_tv_dict\": {str(key): value for key, value in self.multi_hot_tv_dict.items()},\n",
    "            \"partition_times_hot_dict\" : {str(key): value for key, value in self.partition_times_hot_dict.items()}\n",
    "        }\n",
    "        \n",
    "        with open(self.file_path, \"w\") as file:\n",
    "            json.dump(data_to_save, file)\n",
    "        print(\"Dictionaries saved as a JSON file\")\n",
    "        \n",
    "    def load_dicts(self):\n",
    "        with open(self.file_path, \"r\") as file:\n",
    "            loaded_data = json.load(file)\n",
    "        \n",
    "        self.partitioned_tv = {int(key): torch.tensor(value) for key, value in loaded_data[\"partitioned_tv\"].items()}\n",
    "        self.partition_indices_and_one_hot_tv = {int(key): torch.tensor(value) for key, value in loaded_data[\"partition_indices_and_one_hot_tv\"].items()}\n",
    "        self.multi_hot_tv_dict = {int(key): torch.tensor(value) for key, value in loaded_data[\"multi_hot_tv_dict\"].items()}\n",
    "        self.partition_times_hot_dict = {int(key): torch.tensor(value) for key, value in loaded_data[\"partition_times_hot_dict\"].items()}\n",
    "        \n",
    "    def load_ptv_dict(self,path):\n",
    "        with open(path, \"r\") as file:\n",
    "            loaded_data = json.load(file)        \n",
    "        self.pseudo_top_vertices_dict = {int(key): torch.tensor(value) for key, value in loaded_data[\"pseudo_top_vertices_dict\"].items()}\n",
    "        for key in range(self.dimension, len(self.seed_graph.nodes())):\n",
    "            if len(self.pseudo_top_vertices_dict[key]==0):\n",
    "                del self.pseudo_top_vertices_dict[key]\n",
    "\n",
    "    def make_plots(self, dict_name):\n",
    "        d = self.maximum_dimension    \n",
    "        # x axis values \n",
    "        x = range(0, d+1)\n",
    "        print(\"d=\",d)\n",
    "        # corresponding y axis values\n",
    "        if dict_name not in ['partitioned_tv','tv']:\n",
    "            raise ValueError(\"Invalid dictionary name. Must be either partitioned_tv or tv\")\n",
    "        y = list()\n",
    "        if dict_name == 'partitioned_tv':\n",
    "            for key in self._sets.keys():\n",
    "                y.append(len(self._sets[key]))\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.bar(x, y, color='blue') \n",
    "            plt.xlabel('dimension') \n",
    "            plt.xlabel('Number of elements in partition')\n",
    "            plt.title('representative') \n",
    "            plt.xticks(x)\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.savefig('partitionedtv_plot.png')\n",
    "        if dict_name == 'tv':\n",
    "            for key in self.top_vertices_dictionary.keys():\n",
    "                y.append(len(self.top_vertices_dictionary[key]))\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.bar(x, y, color='blue') \n",
    "            plt.xlabel('dimension') \n",
    "            plt.xlabel('Number of partitioned top vertices')\n",
    "            plt.title('dimension distribution') \n",
    "            plt.xticks(x)\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.savefig('partitionedtv_plot.png')          \n",
    "\n",
    "class PartitionError(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de2fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Code testing\"\"\"\n",
    "K_10 = dgl.heterograph({('paper', 'cites', 'paper'): (SimplexCreator(dimension=20).src, SimplexCreator(dimension=20).dst)})\n",
    "filepath = 'K_10'\n",
    "K_10_preprocessing = psuedotv(filepath,graph=K_10,dimension=30)\n",
    "K_10_preprocessing.inductive_connecting()\n",
    "print(\"Top vertices dictionary=\",K_10_preprocessing.top_vertices_dictionary)\n",
    "print(\"Partition by dimension=\",K_10_preprocessing.pure_top_vertices_dict)\n",
    "print(\"top vertex dictionary=\",K_10_preprocessing.top_vertices_dictionary)\n",
    "print(\"Partitions before refinement=\",K_10_preprocessing._partition)\n",
    "K_10_preprocessing.refinement()\n",
    "K_10_preprocessing.add_vertex_features()\n",
    "print(\"Partitions after refinement=\",K_10_preprocessing._partition)\n",
    "print(\"One hot encoding of pure tv=\",K_10_preprocessing.one_hot_dict)\n",
    "print(\"multi hot encoding of all dimensions for tv=\",K_10_preprocessing.multi_hot_tv_dict)\n",
    "print(\"partition indices=\",K_10_preprocessing.partition_index)\n",
    "print(\"tv + partition index hot dict=\",K_10_preprocessing.partition_indices_and_one_hot_tv)\n",
    "print(\"partitioned one-hot encoding, with index 1 if vertex is refined ith time=\",K_10_preprocessing.partitioned_tv)\n",
    "print(\"index of partitions=\",K_10_preprocessing.partition_times_hot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d38df53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished adding 0-top vertices and 1-top vertices in main dictionary\n",
      "Finished adding 0-pseudo top vertices and 1-pseudo top vertices in main dictionary\n",
      "Computing matrices of dimension 2\n",
      "Finished computing matrices for dimension 2\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'pseudo_top_vertices_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marxiv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m arxiv_preprocessing \u001b[38;5;241m=\u001b[39m psuedotv(filepath,graph\u001b[38;5;241m=\u001b[39marxiv_graph,dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43marxiv_preprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_ptv_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msemi_tv_upto5.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 346\u001b[0m, in \u001b[0;36mpsuedotv.load_ptv_dict\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m    345\u001b[0m     loaded_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)        \n\u001b[1;32m--> 346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpseudo_top_vertices_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mint\u001b[39m(key): torch\u001b[38;5;241m.\u001b[39mtensor(value) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mloaded_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpseudo_top_vertices_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdimension, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed_graph\u001b[38;5;241m.\u001b[39mnodes())):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpseudo_top_vertices_dict[key]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[1;31mKeyError\u001b[0m: 'pseudo_top_vertices_dict'"
     ]
    }
   ],
   "source": [
    "dataset = DglNodePropPredDataset(name = \"ogbn-arxiv\", root = 'dataset/')\n",
    "arxiv_graph = dataset.graph[0]\n",
    "filepath = 'arxiv'\n",
    "arxiv_preprocessing = psuedotv(filepath,graph=arxiv_graph,dimension=30)\n",
    "arxiv_preprocessing.load_ptv_dict('semi_tv_upto5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b492a8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af56a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Code testing\"\"\"\n",
    "src_absurd_case4  = [0,0,0,1,1,2] + [1] + [4, 4, 4, 5, 5, 6] \n",
    "dst_absured_case4 = [1,2,3,2,3,3] + [4] + [5, 6, 7, 6, 7, 7]\n",
    "absured_case4 = dgl.heterograph({('paper', 'cites', 'paper'): (src_absurd_case4, dst_absured_case4)})\n",
    "filepath = 'troubling_graph'\n",
    "troubling_graph_preprocessing = psuedotv(filepath,graph=absured_case4,dimension=30)\n",
    "troubling_graph_preprocessing.inductive_connecting() \n",
    "print(\"Top vertices dictionary=\",troubling_graph_preprocessing.top_vertices_dictionary)\n",
    "print(\"Partition by dimension=\",troubling_graph_preprocessing.pure_top_vertices_dict)\n",
    "print(\"top vertex dictionary=\",troubling_graph_preprocessing.top_vertices_dictionary)\n",
    "print(\"Partitions before refinement=\",troubling_graph_preprocessing._partition)\n",
    "troubling_graph_preprocessing.refinement()\n",
    "troubling_graph_preprocessing.add_vertex_features()\n",
    "print(\"Partitions after refinement=\",troubling_graph_preprocessing._partition)\n",
    "print(\"One hot encoding of pure tv=\",troubling_graph_preprocessing.one_hot_dict)\n",
    "print(\"multi hot encoding of all dimensions for tv=\",troubling_graph_preprocessing.multi_hot_tv_dict)\n",
    "print(\"partition indices=\",troubling_graph_preprocessing.partition_index)\n",
    "print(\"tv + partition index hot dict=\",troubling_graph_preprocessing.partition_indices_and_one_hot_tv)\n",
    "print(\"partitioned one-hot encoding, with index 1 if vertex is refined ith time=\",troubling_graph_preprocessing.partitioned_tv)\n",
    "print(\"index of partitions=\",troubling_graph_preprocessing.partition_times_hot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4a1ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_random_graph(num_nodes):\n",
    "    src_edges =[]\n",
    "    dst_edges = []\n",
    "    edges = []\n",
    "    for i in range(2*num_nodes):\n",
    "        src_edges.append(random.randint(0,num_nodes))\n",
    "        dst_edges.append(random.randint(0,num_nodes))\n",
    "        edges.append((src_edges[i],dst_edges[i]))\n",
    "    graph = dgl.heterograph({('paper', 'cites', 'paper'): (src_edges, dst_edges)})\n",
    "    return graph, edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120396ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "dgl_G, edges = generate_random_graph(10)\n",
    "print(edges)\n",
    "nx_G = nx.DiGraph()\n",
    "nx_G.add_edges_from(edges)\n",
    "options = {\n",
    "    'node_color': 'black',\n",
    "    'node_size': 20,\n",
    "    'width': 1,\n",
    "}\n",
    "#pos = nx.spring_layout(nx_G, seed=42)\n",
    "pos = nx.planar_layout(nx_G)\n",
    "nx.draw_networkx(nx_G, pos, with_labels=True, node_color='lightblue', node_size=200, font_size=10, font_color='black', arrows=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dbe128",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath='randomgraph'\n",
    "random_graph_preprocessing = psuedotv(filepath,graph=dgl_G,dimension=30)\n",
    "random_graph_preprocessing.inductive_connecting()\n",
    "print(\"Top vertices dictionary=\",random_graph_preprocessing.top_vertices_dictionary)\n",
    "print(\"Partition by dimension=\",random_graph_preprocessing.pure_top_vertices_dict)\n",
    "print(\"top vertex dictionary=\",random_graph_preprocessing.top_vertices_dictionary)\n",
    "print(\"Partitions before refinement=\",random_graph_preprocessing._partition)\n",
    "random_graph_preprocessing.refinement()\n",
    "random_graph_preprocessing.add_vertex_features()\n",
    "print(\"Partitions after refinement=\",random_graph_preprocessing._partition)\n",
    "print(\"One hot encoding of pure tv=\",random_graph_preprocessing.one_hot_dict)\n",
    "print(\"multi hot encoding of all dimensions for tv=\",random_graph_preprocessing.multi_hot_tv_dict)\n",
    "print(\"partition indices=\",random_graph_preprocessing.partition_index)\n",
    "print(\"tv + partition index hot dict=\",random_graph_preprocessing.partition_indices_and_one_hot_tv)\n",
    "print(\"partitioned one-hot encoding, with index 1 if vertex is refined ith time=\",random_graph_preprocessing.partitioned_tv)\n",
    "print(\"index of partitions=\",random_graph_preprocessing.partition_times_hot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee2932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
