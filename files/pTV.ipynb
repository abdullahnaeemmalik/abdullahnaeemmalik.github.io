{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1520285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from time import process_time\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "from torch.sparse import *\n",
    "import collections\n",
    "os.environ['DGLBACKEND'] = 'pytorch'\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ogb.nodeproppred import DglNodePropPredDataset, Evaluator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0929d",
   "metadata": {},
   "source": [
    "This builds the 1-skeleton of a standard simplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d6fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplexCreator():\n",
    "    \"\"\"Create standard simplex\"\"\"\n",
    "    def __init__(self, dimension):\n",
    "        self.input_dimension = dimension\n",
    "        self.src=list()\n",
    "        self.dst=list()\n",
    "        for i in range(self.input_dimension+1):\n",
    "            for j in range(self.input_dimension+1):\n",
    "                if (i < j):\n",
    "                    self.src = self.src + [i]\n",
    "                    self.dst = self.dst + [j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a745612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class psuedotv():\n",
    "    src=list()\n",
    "    dst=list()\n",
    "    empty_graph = dgl.heterograph({('node', 'to', 'node'): (src, dst)})\n",
    "    dimension = int\n",
    "\n",
    "    assert isinstance(empty_graph, dgl.DGLHeteroGraph), \\\n",
    "        'Keyword argument \\\"graph\\\" of AdjGraph\\'s init methodmust be a dgl.DGLHeteroGraph.'\n",
    "\n",
    "    def __init__(\n",
    "        self, file_path, graph=empty_graph,dimension=dimension):\n",
    "        self.seed_graph = graph\n",
    "        self.srcs_and_dsts = self.seed_graph.edges()   \n",
    "        \n",
    "        \"\"\"Create dictionary with dimension (keys) and place list of nodes with all possible top dimensions (values)\"\"\"\n",
    "        self.top_vertices_dictionary = collections.defaultdict(list)\n",
    "        self.top_vertices_dictionary[0]=[int(x) for x in self.seed_graph.nodes()]\n",
    "        self.top_vertices_dictionary[1]=[int(x) for x in torch.unique(self.srcs_and_dsts[1])]\n",
    "        \"\"\" This dict has keys = dimensions d and values = dictionary. This subdictionary has keys = nodes and values =\n",
    "        vertices that make the node a top-vertex of dimension d\"\"\"\n",
    "\n",
    "        print(\"Finished adding 0-top vertices and 1-top vertices in main dictionary\")\n",
    "        \n",
    "        \"\"\"compute all in_degrees. These are needed for the algorithm later on when the loop runs\"\"\"\n",
    "        self.in_degrees = self.seed_graph.in_degrees()\n",
    "        self.out_degrees = self.seed_graph.out_degrees()\n",
    "        self.dimension = dimension\n",
    "        self.maximum_dimension = int(torch.max(self.in_degrees))\n",
    "        \n",
    "        self.file_path=file_path\n",
    "        \n",
    "        \"\"\"Create dictionary with dimension (keys) and list of nodes with maximum\n",
    "        dimension corresponding to key (values)\"\"\"\n",
    "        self.pseudo_top_vertices_dict = collections.defaultdict(list)\n",
    "        for v in self.seed_graph.nodes():\n",
    "            \"\"\"Add all zero-dimension vertices, and for now keep rest as 1-dimension vertices.\n",
    "            The vertices moved to different keys as more simplices are identified\"\"\"\n",
    "            if self.in_degrees[v] == 0:\n",
    "                self.pseudo_top_vertices_dict[0] = self.pseudo_top_vertices_dict[0] + [int(v)]\n",
    "            else:\n",
    "                self.pseudo_top_vertices_dict[1] = self.pseudo_top_vertices_dict[1] + [int(v)]\n",
    "                \n",
    "        print(\"Finished adding 0-pseudo top vertices and 1-pseudo top vertices in main dictionary\")\n",
    "        \n",
    "        \"\"\"Create empty dictionary as above, but this time will have sets (value) for each dimension (key)\"\"\"        \n",
    "        self.pseudo_top_vertices_dict_of_sets = dict()\n",
    "        \n",
    "        \"\"\"Same values as above, but keys are addresses of the sets\"\"\" \n",
    "        self._sets = dict()\n",
    "        \n",
    "        \"\"\" Creates dictionary from above sets that has representatives (keys) and sets (values)\"\"\"\n",
    "        self._partition = dict()\n",
    "\n",
    "        \"\"\" Needed for refinement. Finds vertices in same class for each iteration of refinement\"\"\"\n",
    "        self.refined_partition = dict()\n",
    "        \n",
    "        \"\"\" Number of refinements done after TV identification.for each vertex. Needed to find final partition index\"\"\"\n",
    "        self.partition_number = 0\n",
    "        \n",
    "        \"\"\" Dictionary with keys = nodes as values = partition index of that node. Currently at zero, since\n",
    "        no refinement is done, yet! The values for each key is supposed to be the partition_number\"\"\"\n",
    "        self.partition_index = {int_node: 0 for int_node in self.top_vertices_dictionary[0]}\n",
    "        \n",
    "        \"\"\"Create dictionary with keys = nodes and values = one-hot tensor of top-vertex and parition index,\n",
    "        both concatenated\"\"\"\n",
    "        self.partition_indices_and_one_hot_tv = {int_node: [] for int_node in self.top_vertices_dictionary[0]}\n",
    "        \n",
    "        \"\"\" Create dictionary with keys = nodes and values = one hot encoding of pseudo top dimension. That is\n",
    "        ith index 1 if top maximum dimension is equal to index and zero otherwise \"\"\"\n",
    "        self.one_hot_dict = collections.defaultdict(list)\n",
    "\n",
    "        \"\"\" Create dictionary with keys = nodes. The values for the dictionary \n",
    "        are tensors which are multi hot encoding with ith index 1 for i-top dimension and zero otherwise \"\"\"\n",
    "        self.multi_hot_tv_dict = collections.defaultdict(list)\n",
    "        \n",
    "        \"\"\" Create dictionary with keys = nodes and values = one hot encoding with ith index 1 if vertex is refined i\n",
    "        times and zero otherwise. This is an index of number of times a vertex has been partitioned. \"\"\"\n",
    "        self.partition_times_hot_dict = collections.defaultdict(list)\n",
    "        \n",
    "        \"\"\" Create dictionary with keys = nodes and values = one hot encoding with ith index 1 if vertex is in the\n",
    "        ith partition and zero otherwise. This captures the number of partitions after refinement. \"\"\"\n",
    "        self.partitioned_tv = collections.defaultdict(list)\n",
    "        \n",
    "        \"\"\" Create a dictionary of vectors R(v) for each vertex v. This gets updated at each refinement process. \n",
    "        The values are stored since then the algorithm wouldn't have to create the vector each time its needed\"\"\"\n",
    "        self.refinement_vectors_dict = collections.defaultdict(list)\n",
    "        \n",
    "        \"\"\" Boolean expression to see if refinement needs to proceed\"\"\"\n",
    "        self.partition_proceeds = True\n",
    "        print(\"Computing matrices of dimension 2\")\n",
    "        diagonal_mask = (self.seed_graph.adj_external()._indices()[0] == self.seed_graph.adj_external()._indices()[1])\n",
    "        off_diagonal_mask = ~diagonal_mask\n",
    "        self.seed_graph.adj_external()._values()[off_diagonal_mask] = 1.0\n",
    "        new_indices = self.seed_graph.adj_external()._indices()[:, off_diagonal_mask]\n",
    "        new_values = self.seed_graph.adj_external()._values()[off_diagonal_mask]\n",
    "        new_size = self.seed_graph.adj_external().size()\n",
    "        new_sparse_matrix = torch.sparse_coo_tensor(indices=new_indices, values=new_values, size=new_size)\n",
    "        self.hadamard_product_prev = new_sparse_matrix\n",
    "        self.adj_matrix_without_diag = new_sparse_matrix\n",
    "        self.powers_of_adj_matrix_witout_diag = new_sparse_matrix\n",
    "        self.hadamard_product_next = torch.mul(new_sparse_matrix,torch.sparse.mm(new_sparse_matrix,new_sparse_matrix))\n",
    "        print(\"Finished computing matrices for dimension 2\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the partition.\"\"\"\n",
    "        return len(self._sets)\n",
    "\n",
    "    def out_nodes_as_int(self, vertex):\n",
    "        \"\"\"convert successors to a list with integer node values\"\"\"\n",
    "        neighbors = [int(v) for v in list(self.seed_graph.successors(vertex))]\n",
    "        if int(vertex) in neighbors:\n",
    "            neighbors.remove(int(vertex))\n",
    "        return neighbors\n",
    "\n",
    "    def in_nodes_as_int(self, vertex):\n",
    "        \"\"\"convert predecessors to a list with integer node values\"\"\"\n",
    "        neighbors = [int(v) for v in list(self.seed_graph.predecessors(vertex))]\n",
    "        if int(vertex) in neighbors:\n",
    "            neighbors.remove(int(vertex))\n",
    "        return neighbors       \n",
    "    \n",
    "    def inductive_connecting(self):\n",
    "        #Case for k=2 doesn't need to search for intersections:\n",
    "        print(\"Adding 2-vertices to the dictionary\")\n",
    "        row_indices_analyzed_node, col_indices_analyzed_node = self.hadamard_product_next._indices()\n",
    "        for vertex in col_indices_analyzed_node.unique():\n",
    "            self.top_vertices_dictionary[2] = self.top_vertices_dictionary[2] + [int(vertex)]\n",
    "            self.pseudo_top_vertices_dict[2] = self.pseudo_top_vertices_dict[2] + [int(vertex)]\n",
    "            if vertex in self.pseudo_top_vertices_dict[1]:\n",
    "                self.pseudo_top_vertices_dict[1].remove(int(vertex))\n",
    "        print(\"Finished adding vertices of dimension 2\")    \n",
    "        \n",
    "        for k in tqdm(range(3,self.dimension+1), position=0, leave=False):\n",
    "            temp_power = torch.sparse.mm(self.powers_of_adj_matrix_witout_diag, self.adj_matrix_without_diag)\n",
    "            diagonal_mask = (temp_power._indices()[0] == temp_power._indices()[1])\n",
    "            off_diagonal_mask = ~diagonal_mask\n",
    "            temp_power._values()[off_diagonal_mask] = 1.0\n",
    "            new_indices = temp_power._indices()[:, off_diagonal_mask]\n",
    "            new_values = temp_power._values()[off_diagonal_mask]\n",
    "            new_size = temp_power.size()\n",
    "            new_sparse_matrix = torch.sparse_coo_tensor(indices=new_indices, values=new_values, size=new_size)\n",
    "            self.powers_of_adj_matrix_no_diag = new_sparse_matrix\n",
    "            #compute A⚬A^2⚬..⚬A^k where ⚬ denotes Hadamard product\n",
    "            self.hadamard_product_prev  = self.hadamard_product_next.clone().detach()\n",
    "            self.hadamard_product_next = torch.mul(self.hadamard_product_prev,self.powers_of_adj_matrix_no_diag)\n",
    "            print(\"Finished computing matrices for dim\",k)\n",
    "            \n",
    "            for v in tqdm(self.seed_graph.nodes(), position=0, leave=False):\n",
    "                if v in self.top_vertices_dictionary[k-1]:\n",
    "                    if self.in_degrees[v] < k:\n",
    "                        continue\n",
    "                    else:\n",
    "                        current_in_neighbors_of_node_analyzed = self.in_nodes_as_int(v)\n",
    "                        for u in tqdm(self.top_vertices_dictionary[k-1], position=0, leave=False):\n",
    "                            if u == v:\n",
    "                                continue\n",
    "                            if u not in current_in_neighbors_of_node_analyzed:\n",
    "                                continue\n",
    "                            else:\n",
    "                                A = self.hadamard_product_prev\n",
    "                                B = self.hadamard_product_next\n",
    "                                A = A.coalesce()\n",
    "                                B = B.coalesce()\n",
    "                                # Get the row and column indices of the nonzero elements in each matrix\n",
    "                                A_row, A_col = A.indices()\n",
    "                                B_row, B_col = B.indices()\n",
    "                                if len(B_col) == 0:\n",
    "                                    print(\"We have reached maximum dimension, and that is\",k-1)\n",
    "                                    self.maximum_dimension = k-1\n",
    "                                    self.top_vertices_dictionary.pop(k, None)\n",
    "                                    return\n",
    "                                # Find the rows that have a nonzero element in the u-th column of A\n",
    "                                A_u_rows = A_row[torch.where(A_col.eq(u))]\n",
    "                                # Find the rows that have a nonzero element in the v-th column of B\n",
    "                                B_v_rows = B_row[torch.where(B_col.eq(v))]\n",
    "                                # Find the common rows between A_u_rows and B_v_rows\n",
    "                                common_rows = np.intersect1d(A_u_rows, B_v_rows)\n",
    "                                #these are all the vertices for which there is a path with unique vertices\n",
    "                                #of length 1, length 2, ..., length k-1 to both u and v\n",
    "                                intersection_criterion = False\n",
    "                                for i in common_rows:\n",
    "                                    intersection = set(self.out_nodes_as_int(i)).intersection(\n",
    "                                        set(self.in_nodes_as_int(u))).intersection(set(current_in_neighbors_of_node_analyzed))\n",
    "                                    if len(intersection) > k-3:\n",
    "                                        intersection_criterion = True\n",
    "                                        break\n",
    "                                if not(intersection_criterion):\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    self.top_vertices_dictionary[k] = self.top_vertices_dictionary[k] + [int(v)]\n",
    "                                    self.pseudo_top_vertices_dict[k] = self.pseudo_top_vertices_dict[k] + [int(v)]\n",
    "                                    if v in self.pseudo_top_vertices_dict[k-1]:\n",
    "                                        self.pseudo_top_vertices_dict[k-1].remove(int(v))\n",
    "                                        \"\"\"a new top vertex v is found, so we can move out of the neighborhood of v\"\"\"\n",
    "                                    break\n",
    "                        \n",
    "            if len(self.top_vertices_dictionary[k]) == 0:\n",
    "                print(\"We have reached maximum dimension, and that is\",k-1)\n",
    "                self.maximum_dimension = k-1\n",
    "                self.top_vertices_dictionary.pop(k, None)\n",
    "                break\n",
    "        print(\"Now creating other initial dictionaries\")\n",
    "        self.pseudo_top_vertices_dict_of_sets = {key:set(self.pseudo_top_vertices_dict[key]) \n",
    "                                                for key in range(0,self.maximum_dimension+1)}          \n",
    "        self._sets = {id(self.pseudo_top_vertices_dict_of_sets[key]):self.pseudo_top_vertices_dict_of_sets[key]\n",
    "                                 for key in self.pseudo_top_vertices_dict_of_sets.keys()}\n",
    "        self._partition = {x:self.pseudo_top_vertices_dict_of_sets[key] \n",
    "                              for key in self.pseudo_top_vertices_dict_of_sets.keys() \n",
    "                           for x in self.pseudo_top_vertices_dict_of_sets[key]}\n",
    "        print(\"Finished creating initial partition\")\n",
    "        print(\"Serializing dictionaries..\")\n",
    "        data_to_save = {\"pseudo_top_vertices_dict\": {str(key): value for key, value in self.pseudo_top_vertices_dict.items()}}\n",
    "        \n",
    "        with open('pseudo_tv.json', \"w\") as file:\n",
    "            json.dump(data_to_save, file)\n",
    "        print(\"Dictionary of pseudo top vertices saved as a JSON file\")\n",
    "    \n",
    "    def partition_vector(self,vertex):\n",
    "        \"\"\" Create vector R(v) for vertex v \"\"\" \n",
    "        vector = [self._partition[vertex]]\n",
    "        for v in torch.sort(self.seed_graph.predecessors(vertex))[0]:\n",
    "            \"\"\" The representatives have to be sorted for a meaningful comparison of vectors \"\"\"\n",
    "            if torch.eq(v,vertex):\n",
    "                pass\n",
    "            else:\n",
    "                vector = vector + [self._partition[int(v)]]\n",
    "        return vector\n",
    "        \n",
    "    def refine(self):\n",
    "        \"\"\"Original idea for refinement algorithm by David Eppstein. \n",
    "        Refine each set A in the partition to the two sets\n",
    "        A & S, A - S.  Also produces output (A & S, A - S)\n",
    "        for each changed set.  Within each pair, A & S will be\n",
    "        a newly created set, while A - S will be a modified\n",
    "        version of an existing set in the partition.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\" hit here is a dictionary with keys = addresses for original partitions and values = vertices with common\n",
    "        partition vector\"\"\"\n",
    "        hit = self.refined_partition\n",
    "        output = list()\n",
    "        for A,AS in hit.items():\n",
    "            A = self._sets[A]\n",
    "            \"\"\"Check if new partition is not the same as the old partition\"\"\"\n",
    "            if AS != A:\n",
    "                self._sets[id(AS)] = AS\n",
    "                \"\"\" This loop finds elements that are not part of previous partition\"\"\"\n",
    "                for x in AS:\n",
    "                    self._partition[x] = AS\n",
    "                \"\"\"The elements that were not part of the partition are now A\"\"\"\n",
    "                A -= AS\n",
    "                output = output + [set.union(A,AS)]\n",
    "        \"\"\" output here keeps track of those partitions that have been broken down\"\"\"\n",
    "        refined_vertices = set().union(*output)\n",
    "        \"\"\"The partitioning process above, once done, should then increase the partition_number, if the above\n",
    "        does indeed count as another genuine refinement. If it does not, then the number of refined_vertices\n",
    "        is zero, and hence should not increase partition number\"\"\"\n",
    "        if len(refined_vertices) == 0:\n",
    "            self.partition_proceeds = False\n",
    "            return\n",
    "        else:\n",
    "            \"\"\"If there is a refinement that takes place, then we increase the partition number and attach\n",
    "            this as the partition index for each vertex in the partition_index dictionary\"\"\"\n",
    "            self.partition_number = self.partition_number+1\n",
    "            for v in refined_vertices:\n",
    "                self.partition_index[v] = self.partition_number\n",
    "\n",
    "    def refinement(self):\n",
    "        \"\"\"Keep on refining the partitions until the partition stabilizes\"\"\"\n",
    "        while self.partition_proceeds:\n",
    "            print(\"Refining..\")\n",
    "            \"\"\"finds vertices u and v such that R(u) = R(v) and make refined partitions here\"\"\"\n",
    "            common_vertices = dict()\n",
    "            for node in tqdm(self.seed_graph.nodes(), position=0, leave=False):\n",
    "                self.refinement_vectors_dict[int(node)] = self.partition_vector(int(node))\n",
    "            print(\"Finished creating list of partition vectors for partition iteration number\",self.partition_number)\n",
    "            print(\"Finding vertices with common partition vectors..\")\n",
    "            \"\"\"This step could be modified for optimization. It needlessly also checks\n",
    "            for partion vectors of vertices that have not been partitioned the first time\"\"\"\n",
    "            for v,u in tqdm(itertools.combinations(self.seed_graph.nodes(),2), position=0, leave=False):\n",
    "                if self.refinement_vectors_dict[int(v)] == self.refinement_vectors_dict[int(u)]:\n",
    "                    Au = self._partition[int(u)]\n",
    "                    common_vertices.setdefault(id(Au),set()).update([int(u),int(v)])\n",
    "            self.refined_partition=common_vertices  \n",
    "            self.refine()\n",
    "        print(\"Refinement process finished\")\n",
    "\n",
    "    def add_vertex_features(self):\n",
    "        \"\"\"First, we start with refinement until the partition stabilizes\"\"\"\n",
    "        sets_for_partition_as_list = list(self._sets.values())\n",
    "        print(\"Adding in node features..\")\n",
    "        for node in tqdm(self.seed_graph.nodes(), position=0, leave=False):\n",
    "            \"\"\" Fills in the following dictionaries\n",
    "            1) multi_hot_tv_dict\n",
    "            2) one_hot_dict\n",
    "            3) partition_indices_and_one_hot_tv\n",
    "            4) partition_times_hot_dict\n",
    "            5) partitioned_tv\n",
    "            by filling in each key (node)\"\"\"\n",
    "            pihvector = [0] * (self.partition_number+1)\n",
    "            mhvector = [0] * (self.maximum_dimension+1)\n",
    "            ohvector = [0] * (self.maximum_dimension+1)\n",
    "            ptvvector = [0] * (len(self._sets))\n",
    "            node = int(node)\n",
    "            for dim in range(0,self.maximum_dimension+1):\n",
    "                if node in self.top_vertices_dictionary[dim]:\n",
    "                    mhvector[dim] = 1\n",
    "                if node in self.pseudo_top_vertices_dict[dim]:\n",
    "                    ohvector[dim] = 1\n",
    "            pihvector[self.partition_index[node]] = 1\n",
    "            self.multi_hot_tv_dict[node] = mhvector\n",
    "            self.one_hot_dict[node] = ohvector\n",
    "            self.partition_times_hot_dict[node] = pihvector\n",
    "            self.partition_indices_and_one_hot_tv[node] = self.one_hot_dict[node] + self.partition_times_hot_dict[node]\n",
    "            index = sets_for_partition_as_list.index(self._partition[node])\n",
    "            ptvvector[index] = 1\n",
    "            self.partitioned_tv[node] = ptvvector\n",
    "        print(\"Vertex features added!\")\n",
    "          \n",
    "    def save_dicts(self):\n",
    "        print(\"Serializing dictionaries for vertex features..\")\n",
    "        data_to_save = {\n",
    "            \"partitioned_tv\": {str(key): value for key, value in self.partitioned_tv.items()},\n",
    "            \"partition_indices_and_one_hot_tv\": {str(key): value for key, value in self.partition_indices_and_one_hot_tv.items()},\n",
    "            \"multi_hot_tv_dict\": {str(key): value for key, value in self.multi_hot_tv_dict.items()},\n",
    "            \"partition_times_hot_dict\" : {str(key): value for key, value in self.partition_times_hot_dict.items()}\n",
    "        }\n",
    "        \n",
    "        with open(self.file_path, \"w\") as file:\n",
    "            json.dump(data_to_save, file)\n",
    "        print(\"Dictionaries saved as a JSON file\")\n",
    "        \n",
    "    def load_dicts(self):\n",
    "        with open(self.file_path, \"r\") as file:\n",
    "            loaded_data = json.load(file)\n",
    "        \n",
    "        self.partitioned_tv = {int(key): torch.tensor(value) for key, value in loaded_data[\"partitioned_tv\"].items()}\n",
    "        self.partition_indices_and_one_hot_tv = {int(key): torch.tensor(value) for key, value in loaded_data[\"partition_indices_and_one_hot_tv\"].items()}\n",
    "        self.multi_hot_tv_dict = {int(key): torch.tensor(value) for key, value in loaded_data[\"multi_hot_tv_dict\"].items()}\n",
    "        self.partition_times_hot_dict = {int(key): torch.tensor(value) for key, value in loaded_data[\"partition_times_hot_dict\"].items()}\n",
    "        \n",
    "    def load_ptv_dict(self,path):\n",
    "        with open(path, \"r\") as file:\n",
    "            loaded_data = json.load(file)        \n",
    "        self.pseudo_top_vertices_dict = {int(key): torch.tensor(value) for key, value in loaded_data[\"pseudo_top_vertices_dict\"].items()}\n",
    "        self.pseudo_top_vertices_dict = {k: v for k, v in self.pseudo_top_vertices_dict.items() \n",
    "                                         if len(v) != 0}        \n",
    "\n",
    "    def make_plots(self, dict_name):\n",
    "        d = self.maximum_dimension    \n",
    "        # x axis values \n",
    "        x = range(0, d+1)\n",
    "        print(\"d=\",d)\n",
    "        # corresponding y axis values\n",
    "        if dict_name not in ['partitioned_tv','tv']:\n",
    "            raise ValueError(\"Invalid dictionary name. Must be either partitioned_tv or tv\")\n",
    "        y = list()\n",
    "        if dict_name == 'partitioned_tv':\n",
    "            for key in self._sets.keys():\n",
    "                y.append(len(self._sets[key]))\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.bar(x, y, color='blue') \n",
    "            plt.xlabel('dimension') \n",
    "            plt.xlabel('Number of elements in partition')\n",
    "            plt.title('representative') \n",
    "            plt.xticks(x)\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.savefig('partitionedtv_plot.png')\n",
    "        if dict_name == 'tv':\n",
    "            for key in self.top_vertices_dictionary.keys():\n",
    "                y.append(len(self.top_vertices_dictionary[key]))\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.bar(x, y, color='blue') \n",
    "            plt.xlabel('dimension') \n",
    "            plt.xlabel('Number of partitioned top vertices')\n",
    "            plt.title('dimension distribution') \n",
    "            plt.xticks(x)\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.savefig('partitionedtv_plot.png')          \n",
    "\n",
    "class PartitionError(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de2fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Code testing\"\"\"\n",
    "K_10 = dgl.heterograph({('paper', 'cites', 'paper'): (SimplexCreator(dimension=20).src, SimplexCreator(dimension=20).dst)})\n",
    "filepath = 'K_10'\n",
    "K_10_preprocessing = psuedotv(filepath,graph=K_10,dimension=30)\n",
    "K_10_preprocessing.inductive_connecting()\n",
    "print(\"Top vertices dictionary=\",K_10_preprocessing.top_vertices_dictionary)\n",
    "print(\"Partition by dimension=\",K_10_preprocessing.pure_top_vertices_dict)\n",
    "print(\"top vertex dictionary=\",K_10_preprocessing.top_vertices_dictionary)\n",
    "print(\"Partitions before refinement=\",K_10_preprocessing._partition)\n",
    "K_10_preprocessing.refinement()\n",
    "K_10_preprocessing.add_vertex_features()\n",
    "print(\"Partitions after refinement=\",K_10_preprocessing._partition)\n",
    "print(\"One hot encoding of pure tv=\",K_10_preprocessing.one_hot_dict)\n",
    "print(\"multi hot encoding of all dimensions for tv=\",K_10_preprocessing.multi_hot_tv_dict)\n",
    "print(\"partition indices=\",K_10_preprocessing.partition_index)\n",
    "print(\"tv + partition index hot dict=\",K_10_preprocessing.partition_indices_and_one_hot_tv)\n",
    "print(\"partitioned one-hot encoding, with index 1 if vertex is refined ith time=\",K_10_preprocessing.partitioned_tv)\n",
    "print(\"index of partitions=\",K_10_preprocessing.partition_times_hot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d38df53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished adding 0-top vertices and 1-top vertices in main dictionary\n",
      "Finished adding 0-pseudo top vertices and 1-pseudo top vertices in main dictionary\n",
      "Computing matrices of dimension 2\n",
      "Finished computing matrices for dimension 2\n"
     ]
    }
   ],
   "source": [
    "dataset = DglNodePropPredDataset(name = \"ogbn-arxiv\", root = 'dataset/')\n",
    "arxiv_graph = dataset.graph[0]\n",
    "filepath = 'arxiv'\n",
    "arxiv_preprocessing = psuedotv(filepath,graph=arxiv_graph,dimension=30)\n",
    "arxiv_preprocessing.load_ptv_dict('semi_tv_upto5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f79b1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 0, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, graph, input_layer:int, hidden_layers:int, output_layer:int, \n",
    "                 num_layers:int, dropout):\n",
    "        # TODO:  Add arguments here needed to define model components\n",
    "        \"\"\"\n",
    "        Defines the architecture of your model.  All layers with learnable parameters should\n",
    "        be created in this method.  The `forward` method will define how to use the \n",
    "        layers created here.\n",
    "        \n",
    "        You will also need to add arguments to the `__init__` method that you need to \n",
    "        create your layers.  For example, you might want to include a `num_layers` argument\n",
    "        so that you can dynamically change the number of layers, and a `dropout` argument \n",
    "        so that this is easy to change.\n",
    "        \"\"\"\n",
    "        self.input_layer   = input_layer\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_layer  = output_layer\n",
    "        self.num_layers    = num_layers\n",
    "        self.dropout       = dropout\n",
    "        self.graph         = graph\n",
    "        super(GCN, self).__init__()\n",
    "        self.convs         = nn.ModuleList()\n",
    "        self.bns           = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_layers))\n",
    "        self.convs.append(GraphConv(input_layer, hidden_layers, norm='both', weight=True, bias=True))\n",
    "        \n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GraphConv(hidden_layers, hidden_layers,\n",
    "                                           norm='both', weight=True, bias=True))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_layers))\n",
    "        self.convs.append(GraphConv(hidden_layers, output_layer, \n",
    "                                    norm='both', weight=True, bias=True))\n",
    "\n",
    "    def forward(self, graph, input_features):\n",
    "        \"\"\"\n",
    "        The forward pass of the model, which applies all of the layers\n",
    "        to a given graph and set of node features\n",
    "        \n",
    "        Args:\n",
    "            g (DGLGraph): the graph used for Graph Convolutions\n",
    "            in_feat (Tensor): the node features\n",
    "        \"\"\"\n",
    "        # TODO: Stack model components to compute the forward pass \n",
    "        for conv in self.convs[:-1]:\n",
    "            input_features = conv(graph, input_features)\n",
    "            input_features = F.relu(input_features)\n",
    "            input_features = F.dropout(input_features, p=self.dropout, training=self.training)\n",
    "        input_features = self.convs[-1](graph, input_features)\n",
    "        return input_features.log_softmax(dim=-1)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "            \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "dataset = DglNodePropPredDataset(name = \"ogbn-arxiv\", root = 'dataset/')\n",
    "arxiv_graph_raw=dataset.graph[0]\n",
    "split_idx = dataset.get_idx_split()\n",
    "labels = dataset.labels.flatten().to(device)\n",
    "val_acc_lb, val_acc_lb_var, test_acc_lb, test_acc_lb_var = 0.7300, 0.0017, 0.7174, 0.0029\n",
    "evaluator = Evaluator(name = \"ogbn-arxiv\")\n",
    "\n",
    "\"\"\"Helper Functions\"\"\"\n",
    "\n",
    "def plot_losses(train_losses, val_losses, modelname, log=False):\n",
    "    \"\"\"\n",
    "    Plots train/validation loss curves vs training epoch\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(train_losses, label='Train')\n",
    "    ax.plot(val_losses, label='Val')\n",
    "    ax.set(xlabel='Epoch', ylabel='CrossEnt')\n",
    "    if log:\n",
    "        ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "    filename = f\"plot_losses_{timestamp,modelname}.png\"\n",
    "    picklename_t = f\"train_losses_{timestamp,modelname}.pkl\"\n",
    "    picklename_l = f\"train_losses_{timestamp,modelname}.pkl\"\n",
    "    plt.savefig(filename)\n",
    "    \n",
    "    with open(picklename_t, 'wb') as file:\n",
    "        pickle.dump(train_losses, file)\n",
    "    \n",
    "    with open(picklename_l, 'wb') as file:\n",
    "        pickle.dump(val_losses, file)\n",
    "        \n",
    "    \n",
    "def train(graph, labels, split_idx, model, epochs, evaluator, \n",
    "          device, save_path, loss_fn=F.cross_entropy, lr=0.01, es_criteria=5, verbose=False):\n",
    "    \"\"\"\n",
    "    A standard interface for model training.  Should be no reason to change this unless you \n",
    "    want to add improvements (e.g., learning rate scheduler).\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "    train_losses = list()\n",
    "    val_losses = list()\n",
    "\n",
    "    features = graph.ndata['feat']\n",
    "    \n",
    "    train_mask = split_idx['train'].to(device)\n",
    "    val_mask = split_idx['valid'].to(device)\n",
    "    test_mask = split_idx['test'].to(device)\n",
    "    es_iters = 0\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        \n",
    "        train_loss, val_loss = train_step(\n",
    "            model, graph, features, labels, train_mask, val_mask, optimizer, loss_fn\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc, val_acc, test_acc = test(model, graph, labels, split_idx, evaluator)\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            es_iters = 0\n",
    "        else:\n",
    "            es_iters += 1\n",
    "            \n",
    "\n",
    "        if e % 50 == 0 and verbose:\n",
    "            print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(\n",
    "                e, train_loss, val_acc, best_val_acc, test_acc, best_test_acc))\n",
    "            \n",
    "        if es_iters >= es_criteria:\n",
    "            print(f\"Early stopping at {e} epochs\")\n",
    "            break\n",
    "            \n",
    "    return np.array(train_losses), np.array(val_losses)\n",
    "\n",
    "def train_step(model, graph, features, labels, train_mask, val_mask, optimizer, loss_fn):\n",
    "    \"\"\"\n",
    "    A single training step\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(graph, features)\n",
    "    loss = loss_fn(logits[train_mask], labels[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_loss = loss_fn(logits[val_mask], labels[val_mask])\n",
    "\n",
    "    return loss.item(), val_loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model_cp, graph, labels, split_idx, evaluator, best_path=None):\n",
    "#def test(model, graph, labels, split_idx, evaluator, best_path=None):\n",
    "    \"\"\"\n",
    "    Executes the OGB Evaluator to return accuracy for \n",
    "    the train, valid and test sets.  If passed in a model file path, \n",
    "    loads the parameters from that file, otherwise uses the model object\n",
    "    passed in.\n",
    "    \"\"\"\n",
    "    model = deepcopy(model_cp)\n",
    "    \n",
    "    if best_path is not None:\n",
    "        model.load_state_dict(torch.load(best_path))\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    features = graph.ndata['feat']\n",
    "    logits = model(graph, features)\n",
    "    y_pred = logits.argmax(1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': labels[split_idx['train']].reshape((-1,1)),\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': labels[split_idx['valid']].reshape((-1,1)),\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': labels[split_idx['test']].reshape((-1,1)),\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "def characterize_performance(model, graph, labels, split_idx, evaluator, best_path, verbose=False):\n",
    "    \"\"\"\n",
    "    Gets performance and compares to the Leaderboard performance for GCN.\n",
    "    Optionally (`verbose=True`) will put the performance in context with the variation\n",
    "    reported on the Leaderboard and indicate whether performance is above/below \n",
    "    1-standard deviation from the mean, as given by Leaderboard.\n",
    "    \"\"\"\n",
    "    train_acc, val_acc, test_acc = test(model, graph, labels, split_idx, evaluator, best_path)\n",
    "    print(\n",
    "        f\"Leaderboard:  Test Acc={test_acc_lb} +/- {test_acc_lb_var}, Val Acc={val_acc_lb} +/- {val_acc_lb_var}\\n\"\n",
    "        f\"Yours:        Test Acc={test_acc:.4f},            Val Acc={val_acc:.4f}\\n\"\n",
    "    )\n",
    "\n",
    "    val_lb = val_acc_lb - val_acc_lb_var\n",
    "    val_ub = val_acc_lb + val_acc_lb_var\n",
    "    \n",
    "    if verbose:\n",
    "        if not val_acc >= val_lb:\n",
    "            print(\n",
    "                f\"Validation performance is worse than LB.  Expected lower bound of {val_lb:.4f}, but got {val_acc:.4f}.\")\n",
    "        elif val_acc > val_ub:\n",
    "            print(\n",
    "                f\"Validation performance is better than LB.  Expected upper bound of {val_ub:.4f}, but got {val_acc:.4f}.\")\n",
    "        else: \n",
    "            print(\n",
    "                f\"Validation performance is in the expected range of {val_lb} - {val_ub}.\"\n",
    "            )\n",
    "    \n",
    "    test_lb = test_acc_lb - test_acc_lb_var\n",
    "    test_ub = test_acc_lb + test_acc_lb_var\n",
    "    if verbose:\n",
    "        if not test_acc >= test_lb:\n",
    "            print(\n",
    "                f\"Test performance is worse than LB.  Expected lower bound of {test_lb:.4f}, but got {test_acc:.4f}.\")\n",
    "\n",
    "        elif test_acc > test_ub:\n",
    "            print(\n",
    "                f\"Test performance is better than LB.  Expected upper bound of {test_ub:.4f}, but got {test_acc:.4f}.\")\n",
    "        else:\n",
    "            print(f\"Test performance is in the expected range of {test_lb} - {test_ub}.\")\n",
    "        \n",
    "    return val_acc, test_acc\n",
    "\n",
    "def norm_plot(curves, title):\n",
    "    \"\"\"\n",
    "    Plots normal distribution curves\n",
    "    curves: list of tuples like: (mu, sigma, label)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    for mu, sigma, label in curves:\n",
    "        x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "        ax.plot(x, stats.norm.pdf(x, mu, sigma), label=label)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "    filename = f\"norm_plot_{timestamp}.png\"\n",
    "    plt.savefig(filename)\n",
    "    \n",
    "def get_experiment_stats(model_cls, model_args, train_args, n_experiments=10):\n",
    "    \"\"\"\n",
    "    Runs an experiment multiple times to get a measure of variation\n",
    "    \"\"\"\n",
    "    results = dict()\n",
    "    for i in range(n_experiments):\n",
    "        model = model_cls(**model_args).to(train_args['device'])\n",
    "        print(f\"Starting training for experiment {i+1}\")\n",
    "        # Add experiment number to model save_path\n",
    "        train_args_cp = deepcopy(train_args)\n",
    "        save_path, file_ext = train_args_cp.pop('save_path').split('.')\n",
    "        timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "        save_path_mod = f\"{save_path}__{timestamp}_{i}.{file_ext}\"\n",
    "        \n",
    "        train_losses, val_losses = train(model=model, save_path=save_path_mod, **train_args_cp)\n",
    "        val_acc, test_acc = characterize_performance(\n",
    "            model, train_args['g_simple'], train_args['labels'], train_args['split_idx'], \n",
    "            train_args['evaluator'], save_path_mod, train_args.get('verbose', False))\n",
    "        \n",
    "        results[i] = dict(val_acc=val_acc, test_acc=test_acc)\n",
    "        print(\"Training complete\\n\")\n",
    "        \n",
    "    df_stats = pd.DataFrame(results).T.agg(['mean', 'std'])\n",
    "    return df_stats\n",
    "    \n",
    "file_name = \"psuedotvs.json\"\n",
    "arxiv_preprocessing_d5 = psuedotv(file_name,graph=arxiv_graph_raw,dimension=5)\n",
    "arxiv_preprocessing.load_ptv_dict('Downloads/semi_tv_upto5.json')\n",
    "arxiv_preprocessing_d5.refinement()\n",
    "print(\"Creating vertex features for the graph now\")\n",
    "arxiv_preprocessing_d5.add_vertex_features()\n",
    "arxiv_preprocessing_d5.save_dicts()\n",
    "\n",
    "def add_features_to_graph(graph,features):\n",
    "    \"\"\"All the existing features and new features need to be stacked and then replaced\n",
    "    with existing features. DGL graph library doesn't support adding node features node-wise\n",
    "    Returns the graph\"\"\"\n",
    "    if not \"feat\" in graph.ndata:\n",
    "        print(\"Error! The graph should have node features called 'feat'.\")\n",
    "        return\n",
    "    else:\n",
    "        existing_node_features = graph.ndata['feat']\n",
    "    \n",
    "    if not (type(features) == dict or type(features) == collections.defaultdict):\n",
    "        print(\"Error! Features should be a dictionary\")\n",
    "        return\n",
    "    stacked_features = torch.stack(list(features.values()))\n",
    "    # Iterate through nodes and concatenate stacked_features to existing_node_features\n",
    "    concatenated_features = []\n",
    "    for node_id in graph.nodes():\n",
    "        concatenated_feature = torch.cat((existing_node_features[node_id], stacked_features[node_id]), dim=0)\n",
    "        concatenated_features.append(concatenated_feature)\n",
    "\n",
    "    # Convert concatenated_features to a tensor\n",
    "    concatenated_features = torch.stack(concatenated_features, dim=0)\n",
    "\n",
    "    # Update the node features in the DGL graph\n",
    "    graph.ndata['feat'] = concatenated_features\n",
    "    \n",
    "    return graph\n",
    "\n",
    "print(\"Creating six different graphs..\")\t\n",
    "arxiv_graph_bidirected = dgl.to_bidirected(arxiv_graph_raw)\n",
    "arxiv_graph_bidirected = dgl.add_self_loop(arxiv_graph)\n",
    "arxiv_graph_bidirected.ndata['feat'] = arxiv_graph_raw.ndata['feat']\n",
    "\n",
    "arxiv_graph = dgl.add_self_loop(arxiv_graph_raw)\n",
    "arxiv_graph.ndata['feat'] = arxiv_graph_raw.ndata['feat']\n",
    "\n",
    "arxiv_graph_bidirected_mixed_tv_d5 = add_features_to_graph(arxiv_graph_bidirected,arxiv_preprocessing_d5.multi_hot_tv_dict)\n",
    "arxiv_graph_bidirected_tv_and_pi_d5 = add_features_to_graph(arxiv_graph_bidirected,arxiv_preprocessing_d5.partition_indices_and_one_hot_tv)\n",
    "arxiv_graph_bidirected_refined_d5 = add_features_to_graph(arxiv_graph_bidirected,arxiv_preprocessing_d5.partitioned_tv)\n",
    "\n",
    "arxiv_graph_mixed_tv_d5 = add_features_to_graph(arxiv_graph,arxiv_preprocessing_d5.multi_hot_tv_dict)\n",
    "arxiv_graph_tv_and_pi_d5 = add_features_to_graph(arxiv_graph,arxiv_preprocessing_d5.partition_indices_and_one_hot_tv)\n",
    "arxiv_graph_refined_d5 = add_features_to_graph(arxiv_graph,arxiv_preprocessing_d5.partitioned_tv)\n",
    "\n",
    "arxiv_graph_bidirected_mixed_tv_d5.ndata['year'] = arxiv_graph_raw.ndata['year']\n",
    "arxiv_graph_bidirected_tv_and_pi_d5.ndata['year'] = arxiv_graph_raw.ndata['year']\n",
    "arxiv_graph_bidirected_refined_d5.ndata['year'] = arxiv_graph_raw.ndata['year']\n",
    "arxiv_graph_mixed_tv_d5.ndata['year'] = arxiv_graph_raw.ndata['year']\n",
    "arxiv_graph_tv_and_pi_d5.ndata['year'] = arxiv_graph_raw.ndata['year']\n",
    "arxiv_graph_refined_d5.ndata['year'] = arxiv_graph_raw.ndata['year']\n",
    "print(\"Graphs made\")\n",
    "\n",
    "def getdim_inlayer(graph):\n",
    "    return graph.ndata['feat'].size()[1]\n",
    "\t\n",
    "model1_mixedtv_kwargs_d5 = dict(graph=arxiv_graph_mixed_tv_d5, input_layer=getdim_inlayer(arxiv_graph_mixed_tv_d5), \n",
    "                     hidden_layers=256, output_layer=40, \n",
    "                 num_layers=3, dropout=0.5)\n",
    "\n",
    "model_mixedtv_d5 = GCN(**model1_mixedtv_kwargs_d5).to(device)\n",
    "\n",
    "model1_tv_and_pi_kwargs_d5 = dict(graph=arxiv_graph_tv_and_pi_d5, input_layer=getdim_inlayer(arxiv_graph_tv_and_pi_d5), \n",
    "                       hidden_layers=256, output_layer=40, \n",
    "                 num_layers=3, dropout=0.5)\n",
    "\n",
    "model1_tv_and_pi_d5 = GCN(**model1_tv_and_pi_kwargs_d5).to(device)\n",
    "\n",
    "model1_refined_tv_kwargs_d5 = dict(graph=arxiv_graph_refined_d5, input_layer=getdim_inlayer(arxiv_graph_refined_d5),\n",
    "                         hidden_layers=256, output_layer=40, \n",
    "                 num_layers=3, dropout=0.5)\n",
    "\n",
    "model1_refined_tv_d5 = GCN(**model1_refined_tv_kwargs_d5).to(device)\n",
    "\n",
    "model2_mixedtv_kwargs_d5 = dict(graph=arxiv_graph_bidirected_mixed_tv_d5, input_layer=getdim_inlayer(arxiv_graph_bidirected_mixed_tv_d5), \n",
    "                     hidden_layers=256, output_layer=40, \n",
    "                 num_layers=3, dropout=0.5)\n",
    "\n",
    "model2_mixedtv_d5 = GCN(**model2_mixedtv_kwargs_d5).to(device)\n",
    "\n",
    "model2_tv_and_pi_kwargs_d5 = dict(graph=arxiv_graph_bidirected_tv_and_pi_d5, input_layer=getdim_inlayer(arxiv_graph_bidirected_tv_and_pi_d5), \n",
    "                       hidden_layers=256, output_layer=40, \n",
    "                 num_layers=3, dropout=0.5)\n",
    "\n",
    "model2_tv_and_pi_d5 = GCN(**model2_tv_and_pi_kwargs_d5).to(device)\n",
    "\n",
    "model2_refined_tv_kwargs_d5 = dict(graph=arxiv_graph_bidirected_refined_d5, input_layer=getdim_inlayer(arxiv_graph_bidirected_refined_d5),\n",
    "                         hidden_layers=256, output_layer=40, \n",
    "                 num_layers=3, dropout=0.5)\n",
    "\n",
    "model2_refined_tv_d5 = GCN(**model2_refined_tv_kwargs_d5).to(device)\n",
    "\n",
    "model1_tv_and_pi_path_d5 = 'model1_tv_and_pi_d5'\n",
    "Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "gcn1_tv_and_pi_path_d5 = f\"{model1_tv_and_pi_path_d5}/gcn_base.model\"\n",
    "\n",
    "model1_mixed_tv_path_d5 = 'model1_mixed_tv_d5'\n",
    "Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "gcn1_mixed_tv_path_d5 = f\"{model1_mixed_tv_path_d5}/gcn_base.model\"\n",
    "\n",
    "model1_refined_tv_path_d5 = 'model1_refined_tv_d5'\n",
    "Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "gcn1_refined_tv_path_d5 = f\"{model1_refined_tv_path_d5}/gcn_base.model\"\n",
    "\n",
    "model2_tv_and_pi_path_d5 = 'model1_tv_and_pi_d5'\n",
    "Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "gcn2_tv_and_pi_path_d5 = f\"{model2_tv_and_pi_path_d5}/gcn_base.model\"\n",
    "\n",
    "model2_mixed_tv_path_d5 = 'model1_mixed_tv_d5'\n",
    "Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "gcn2_mixed_tv_path_d5 = f\"{model2_mixed_tv_path_d5}/gcn_base.model\"\n",
    "\n",
    "model2_refined_tv_path_d5 = 'model2_refined_tv_d5'\n",
    "Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "gcn2_refined_tv_path_d5 = f\"{model2_refined_tv_path_d5}/gcn_base.model\"\n",
    "\n",
    "train_args_1_tv_and_pi_d5 = dict(\n",
    "    graph=arxiv_graph_tv_and_pi_d5, labels=labels, split_idx=split_idx, \n",
    "    epochs=500, evaluator=evaluator, device=device, \n",
    "    save_path=gcn1_tv_and_pi_path_d5, lr=5e-3, es_criteria=50,\n",
    ")\n",
    "\n",
    "train_args_1_mixed_tv_d5 = dict(\n",
    "    graph=arxiv_graph_mixed_tv_d5, labels=labels, split_idx=split_idx, \n",
    "    epochs=500, evaluator=evaluator, device=device, \n",
    "    save_path=gcn1_mixed_tv_path_d5, lr=5e-3, es_criteria=50,\n",
    ")\n",
    "\n",
    "train_args_1_refined_tv_d5 = dict(\n",
    "    graph=arxiv_graph_refined_d5, labels=labels, split_idx=split_idx, \n",
    "    epochs=500, evaluator=evaluator, device=device, \n",
    "    save_path=gcn1_refined_tv_path_d5, lr=5e-3, es_criteria=50,\n",
    ")\n",
    "\n",
    "train_args_2_tv_and_pi_d5 = dict(\n",
    "    graph=arxiv_graph_bidirected_tv_and_pi_d5, labels=labels, split_idx=split_idx, \n",
    "    epochs=500, evaluator=evaluator, device=device, \n",
    "    save_path=gcn2_tv_and_pi_path_d5, lr=5e-3, es_criteria=50,\n",
    ")\n",
    "\n",
    "train_args_2_mixed_tv_d5 = dict(\n",
    "    graph=arxiv_graph_bidirected_mixed_tv_d5, labels=labels, split_idx=split_idx, \n",
    "    epochs=500, evaluator=evaluator, device=device, \n",
    "    save_path=gcn2_mixed_tv_path_d5, lr=5e-3, es_criteria=50,\n",
    ")\n",
    "\n",
    "train_args_2_refined_tv_d5 = dict(\n",
    "    graph=arxiv_graph_bidirected_refined_d5, labels=labels, split_idx=split_idx, \n",
    "    epochs=500, evaluator=evaluator, device=device, \n",
    "    save_path=gcn2_refined_tv_path_d5, lr=5e-3, es_criteria=50,\n",
    ")\n",
    "print(\"Training for directed graph with mixed tv features..\")\n",
    "train_losses1_mixedtv_d5, val_losses1_mixedtv_d5 = train(model=model_mixedtv_d5, verbose=True, **train_args_1_mixed_tv_d5)\n",
    "print(\"Training for directed graph with tv and pi..\")\n",
    "train_losses1_tv_and_pi_d5, val_losses1_tv_and_pi_d5 = train(model=model1_tv_and_pi_d5, verbose=True, **train_args_1_tv_and_pi_d5)\n",
    "print(\"Training for directed graph with refined tv..\")\n",
    "train_losses1_refined_tv_d5, val_losses1_refined_tv_d5 = train(model=model1_refined_tv_d5, verbose=True, **train_args_1_refined_tv_d5)\n",
    "print(\"Training for undirected graph with mixed tv..\")\n",
    "train_losses2_mixedtv_d5, val_losses2_mixedtv_d5 = train(model=model2_mixedtv_d5, verbose=True, **train_args_2_mixed_tv_d5)\n",
    "print(\"Training for undirected graph with tv and pi..\")\n",
    "train_losses2_tv_and_pi_d5, val_losses2_tv_and_pi_d5 = train(model=mode12_tv_and_pi_d5, verbose=True, **train_args_2_tv_and_pi_d5)\n",
    "print(\"Training for undirected graph with refined tv..\")\n",
    "train_losses2_refined_tv_d5, val_losses2_refined_tv_d5 = train(model=model2_refined_tv_d5, verbose=True, **train_args_2_refined_tv_d5)\n",
    "\n",
    "print(\"Creating plots..\")\n",
    "plot_losses(train_losses1_mixedtv_d5, val_losses1_mixedtv_d5, log=True, modelname='1_mixedtv_d5')\n",
    "plot_losses(train_losses1_tv_and_pi_d5, train_losses1_tv_and_pi_d5, log=True, modelname='1_tv_and_pi_3')\n",
    "plot_losses(train_losses1_refined_tv_d5, train_losses1_refined_tv_d5, log=True, modelname='1_refined_tv_d5')\n",
    "plot_losses(train_losses2_mixedtv_d5, val_losses2_mixedtv_d5, log=True, modelname='2_mixedtv_d5')\n",
    "plot_losses(train_losses2_tv_and_pi_d5, train_losses2_tv_and_pi_d5, log=True, modelname='2_tv_and_pi_3')\n",
    "plot_losses(train_losses2_refined_tv_d5, train_losses2_refined_tv_d5, log=True, modelname='2_refined_tv_d5')\n",
    "\n",
    "print(\"Characterizing performance of the model for directed graph with tv and pi\")\n",
    "characterize_performance(model1_tv_and_pi_d5, arxiv_graph_tv_and_pi_d5, labels, split_idx, evaluator, gcn_path, verbose=True)\n",
    "print(\"Characterizing performance of the model for directed graph with mixed tv\")\n",
    "characterize_performance(model_mixedtv_d5, arxiv_graph_mixed_tv_d5, labels, split_idx, evaluator, gcn_path, verbose=True)\n",
    "print(\"Characterizing performance of the model for directed graph with refined tv\")\n",
    "characterize_performance(model1_refined_tv_d5, arxiv_graph_refined_d5, labels, split_idx, evaluator, gcn_path, verbose=True)\n",
    "print(\"Characterizing performance of the model for undirected graph with tv and pi\")\n",
    "characterize_performance(model2_tv_and_pi_d5, arxiv_graph_bidirected_tv_and_pi_d5, labels, split_idx, evaluator, gcn_path, verbose=True)\n",
    "print(\"Characterizing performance of the model for directed graph with mixed tv\")\n",
    "characterize_performance(mode2_mixedtv_d5, arxiv_graph_bidirected_mixed_tv_d5, labels, split_idx, evaluator, gcn_path, verbose=True)\n",
    "print(\"Characterizing performance of the model for directed graph with refined tv\")\n",
    "characterize_performance(model2_refined_tv_d5, arxiv_graph_bidirected_refined_d5, labels, split_idx, evaluator, gcn_path, verbose=True)\n",
    "\n",
    "print(\"Beginning training with 10 experiments of the model for directed graph with mixed tv..\")\n",
    "df_gcn1_mixedtv_d5 = get_experiment_stats(\n",
    "    model_cls=GCN, model_args=model1_mixedtv_kwargs_d5,\n",
    "    train_args=train_args, n_experiments=10\n",
    ")\n",
    "\n",
    "print(\"Beginning training with 10 experiments of the model for directed graph with tv and pi\")\n",
    "df_gcn1_tv_and_pi_d5 = get_experiment_stats(\n",
    "    model_cls=GCN, model_args=model1_tv_and_pi_kwargs_d5,\n",
    "    train_args=train_args, n_experiments=10\n",
    ")\n",
    "\n",
    "print(\"Beginning training with 10 experiments of the model for directed graph with refined tv\")\n",
    "df_gcn1_refined_tv_d5 = get_experiment_stats(\n",
    "    model_cls=GCN, model_args=model1_refined_tv_kwargs_d5,\n",
    "    train_args=train_args, n_experiments=10\n",
    ")\n",
    "\n",
    "print(\"Beginning training with 10 experiments of the model for undirected graph with mixed tv\")\n",
    "df_gcn2_mixedtv_d5 = get_experiment_stats(\n",
    "    model_cls=GCN, model_args=model2_mixedtv_kwargs_d5,\n",
    "    train_args=train_args, n_experiments=10\n",
    ")\n",
    "\n",
    "print(\"Beginning training with 10 experiments of the model for directed graph with tv and pi\")\n",
    "df_gcn2_tv_and_pi_d5 = get_experiment_stats(\n",
    "    model_cls=GCN, model_args=model2_tv_and_pi_kwargs_d5,\n",
    "    train_args=train_args, n_experiments=10\n",
    ")\n",
    "\n",
    "print(\"Beginning training with 10 experiments of the model for directed graph with refined tv\")\n",
    "df_gcn2_refined_tv_d5 = get_experiment_stats(\n",
    "    model_cls=GCN, model_args=model2_refined_tv_kwargs_d5,\n",
    "    train_args=train_args, n_experiments=10\n",
    ")\n",
    "\n",
    "print(\"Beginning training with 10 experiments of the model for directed graph with tv and pi\")\n",
    "norm_plot(\n",
    "    [\n",
    "        (test_acc_lb, test_acc_lb_var, 'Leaderboard'), \n",
    "        (df_gcn1_tv_and_pi_d5.loc['mean', 'test_acc'], df_gcn1_tv_and_pi_d5.loc['std', 'test_acc'], 'GCN'),\n",
    "    ],\n",
    "    'Test Performance'\n",
    ")\n",
    "print(\"Creating norm plot for directed graph with mixed tv\")\n",
    "norm_plot(\n",
    "    [\n",
    "        (test_acc_lb, test_acc_lb_var, 'Leaderboard'), \n",
    "        (df_gcn1_mixedtv_d5.loc['mean', 'test_acc'], df_gcn1_mixedtv_d5.loc['std', 'test_acc'], 'GCN'),\n",
    "    ],\n",
    "    'Test Performance'\n",
    ")\n",
    "print(\"Creating norm plot for directed graph with refined tv\")\n",
    "norm_plot(\n",
    "    [\n",
    "        (test_acc_lb, test_acc_lb_var, 'Leaderboard'), \n",
    "        (df_gcn1_refined_tv_d5.loc['mean', 'test_acc'], df_gcn1_refined_tv_d5.loc['std', 'test_acc'], 'GCN'),\n",
    "    ],\n",
    "    'Test Performance'\n",
    ")\n",
    "print(\"Creating norm plot for undirected graph with tv and pi\")\n",
    "norm_plot(\n",
    "    [\n",
    "        (test_acc_lb, test_acc_lb_var, 'Leaderboard'), \n",
    "        (df_gcn2_tv_and_pi_d5.loc['mean', 'test_acc'], df_gcn2_tv_and_pi_d5.loc['std', 'test_acc'], 'GCN'),\n",
    "    ],\n",
    "    'Test Performance'\n",
    ")\n",
    "print(\"Creating norm plot for undirected graph with mixed tv\")\n",
    "norm_plot(\n",
    "    [\n",
    "        (test_acc_lb, test_acc_lb_var, 'Leaderboard'), \n",
    "        (df_gcn2_mixedtv_d5.loc['mean', 'test_acc'], df_gcn2_mixedtv_d5.loc['std', 'test_acc'], 'GCN'),\n",
    "    ],\n",
    "    'Test Performance'\n",
    ")\n",
    "print(\"Creating norm plot for undirected graph with refined tv\")\n",
    "norm_plot(\n",
    "    [\n",
    "        (test_acc_lb, test_acc_lb_var, 'Leaderboard'), \n",
    "        (df_gcn2_refined_tv_d5.loc['mean', 'test_acc'], df_gcn2_refined_tv_d5.loc['std', 'test_acc'], 'GCN'),\n",
    "    ],\n",
    "    'Test Performance'\n",
    ")\n",
    "\n",
    "print(\"Evaluating the model for directed graph with refined tv\")\n",
    "_, p = stats.ttest_ind_from_stats(\n",
    "    test_acc_lb, test_acc_lb_var, 10,\n",
    "    df_gcn1_refined_tv_d5.loc['mean', 'test_acc'], df_gcn1_refined_tv_d5.loc['std', 'test_acc'], 10,\n",
    "    equal_var=False,\n",
    ")\n",
    "print(f\"Mean Test Accuracy Improvement: {(df_gcn1_refined_tv_d5.loc['mean', 'test_acc'] - test_acc_lb):.4f}\")\n",
    "print(f\"Probability that these are from the same performance distribution = {p*100:.0f}%\")\n",
    "\n",
    "print(\"Evaluating the model for directed graph with mixed tv\")\n",
    "_, p = stats.ttest_ind_from_stats(\n",
    "    test_acc_lb, test_acc_lb_var, 10,\n",
    "    df_gcn1_mixedtv_d5.loc['mean', 'test_acc'], df_gcn1_mixedtv_d5.loc['std', 'test_acc'], 10,\n",
    "    equal_var=False,\n",
    ")\n",
    "print(f\"Mean Test Accuracy Improvement: {(df_gcn1_mixedtv_d5.loc['mean', 'test_acc'] - test_acc_lb):.4f}\")\n",
    "print(f\"Probability that these are from the same performance distribution = {p*100:.0f}%\")\n",
    "\n",
    "print(\"Evaluating the model for directed graph with tv and pi\")\n",
    "_, p = stats.ttest_ind_from_stats(\n",
    "    test_acc_lb, test_acc_lb_var, 10,\n",
    "    df_gcn1_tv_and_pi_d5.loc['mean', 'test_acc'], df_gcn1_tv_and_pi_d5.loc['std', 'test_acc'], 10,\n",
    "    equal_var=False,\n",
    ")\n",
    "print(f\"Mean Test Accuracy Improvement: {(df_gcn1_tv_and_pi_d5.loc['mean', 'test_acc'] - test_acc_lb):.4f}\")\n",
    "print(f\"Probability that these are from the same performance distribution = {p*100:.0f}%\")\n",
    "\n",
    "print(\"Evaluating the model for undirected graph with refined tv\")\n",
    "_, p = stats.ttest_ind_from_stats(\n",
    "    test_acc_lb, test_acc_lb_var, 10,\n",
    "    df_gcn2_refined_tv_d5.loc['mean', 'test_acc'], df_gcn2_refined_tv_d5.loc['std', 'test_acc'], 10,\n",
    "    equal_var=False,\n",
    ")\n",
    "print(f\"Mean Test Accuracy Improvement: {(df_gcn2_refined_tv_d5.loc['mean', 'test_acc'] - test_acc_lb):.4f}\")\n",
    "print(f\"Probability that these are from the same performance distribution = {p*100:.0f}%\")\n",
    "\n",
    "print(\"Evaluating the model for undirected graph with mixed tv\")\n",
    "_, p = stats.ttest_ind_from_stats(\n",
    "    test_acc_lb, test_acc_lb_var, 10,\n",
    "    df_gcn2_mixedtv_d5.loc['mean', 'test_acc'], df_gcn2_mixedtv_d5.loc['std', 'test_acc'], 10,\n",
    "    equal_var=False,\n",
    ")\n",
    "print(f\"Mean Test Accuracy Improvement: {(df_gcn2_mixedtv_d5.loc['mean', 'test_acc'] - test_acc_lb):.4f}\")\n",
    "print(f\"Probability that these are from the same performance distribution = {p*100:.0f}%\")\n",
    "\n",
    "print(\"Evaluating the model for undirected graph with tv and pi\")\n",
    "_, p = stats.ttest_ind_from_stats(\n",
    "    test_acc_lb, test_acc_lb_var, 10,\n",
    "    df_gcn2_tv_and_pi_d5.loc['mean', 'test_acc'], df_gcn2_tv_and_pi_d5.loc['std', 'test_acc'], 10,\n",
    "    equal_var=False,\n",
    ")\n",
    "print(f\"Mean Test Accuracy Improvement: {(df_gcn2_tv_and_pi_d5.loc['mean', 'test_acc'] - test_acc_lb):.4f}\")\n",
    "print(f\"Probability that these are from the same performance distribution = {p*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af56a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Code testing\"\"\"\n",
    "src_absurd_case4  = [0,0,0,1,1,2] + [1] + [4, 4, 4, 5, 5, 6] \n",
    "dst_absured_case4 = [1,2,3,2,3,3] + [4] + [5, 6, 7, 6, 7, 7]\n",
    "absured_case4 = dgl.heterograph({('paper', 'cites', 'paper'): (src_absurd_case4, dst_absured_case4)})\n",
    "filepath = 'troubling_graph'\n",
    "troubling_graph_preprocessing = psuedotv(filepath,graph=absured_case4,dimension=30)\n",
    "troubling_graph_preprocessing.inductive_connecting() \n",
    "print(\"Top vertices dictionary=\",troubling_graph_preprocessing.top_vertices_dictionary)\n",
    "print(\"Partition by dimension=\",troubling_graph_preprocessing.pure_top_vertices_dict)\n",
    "print(\"top vertex dictionary=\",troubling_graph_preprocessing.top_vertices_dictionary)\n",
    "print(\"Partitions before refinement=\",troubling_graph_preprocessing._partition)\n",
    "troubling_graph_preprocessing.refinement()\n",
    "troubling_graph_preprocessing.add_vertex_features()\n",
    "print(\"Partitions after refinement=\",troubling_graph_preprocessing._partition)\n",
    "print(\"One hot encoding of pure tv=\",troubling_graph_preprocessing.one_hot_dict)\n",
    "print(\"multi hot encoding of all dimensions for tv=\",troubling_graph_preprocessing.multi_hot_tv_dict)\n",
    "print(\"partition indices=\",troubling_graph_preprocessing.partition_index)\n",
    "print(\"tv + partition index hot dict=\",troubling_graph_preprocessing.partition_indices_and_one_hot_tv)\n",
    "print(\"partitioned one-hot encoding, with index 1 if vertex is refined ith time=\",troubling_graph_preprocessing.partitioned_tv)\n",
    "print(\"index of partitions=\",troubling_graph_preprocessing.partition_times_hot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4a1ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_random_graph(num_nodes):\n",
    "    src_edges =[]\n",
    "    dst_edges = []\n",
    "    edges = []\n",
    "    for i in range(2*num_nodes):\n",
    "        src_edges.append(random.randint(0,num_nodes))\n",
    "        dst_edges.append(random.randint(0,num_nodes))\n",
    "        edges.append((src_edges[i],dst_edges[i]))\n",
    "    graph = dgl.heterograph({('paper', 'cites', 'paper'): (src_edges, dst_edges)})\n",
    "    return graph, edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120396ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "dgl_G, edges = generate_random_graph(10)\n",
    "print(edges)\n",
    "nx_G = nx.DiGraph()\n",
    "nx_G.add_edges_from(edges)\n",
    "options = {\n",
    "    'node_color': 'black',\n",
    "    'node_size': 20,\n",
    "    'width': 1,\n",
    "}\n",
    "#pos = nx.spring_layout(nx_G, seed=42)\n",
    "pos = nx.planar_layout(nx_G)\n",
    "nx.draw_networkx(nx_G, pos, with_labels=True, node_color='lightblue', node_size=200, font_size=10, font_color='black', arrows=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dbe128",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath='randomgraph'\n",
    "random_graph_preprocessing = psuedotv(filepath,graph=dgl_G,dimension=30)\n",
    "random_graph_preprocessing.inductive_connecting()\n",
    "print(\"Top vertices dictionary=\",random_graph_preprocessing.top_vertices_dictionary)\n",
    "print(\"Partition by dimension=\",random_graph_preprocessing.pure_top_vertices_dict)\n",
    "print(\"top vertex dictionary=\",random_graph_preprocessing.top_vertices_dictionary)\n",
    "print(\"Partitions before refinement=\",random_graph_preprocessing._partition)\n",
    "random_graph_preprocessing.refinement()\n",
    "random_graph_preprocessing.add_vertex_features()\n",
    "print(\"Partitions after refinement=\",random_graph_preprocessing._partition)\n",
    "print(\"One hot encoding of pure tv=\",random_graph_preprocessing.one_hot_dict)\n",
    "print(\"multi hot encoding of all dimensions for tv=\",random_graph_preprocessing.multi_hot_tv_dict)\n",
    "print(\"partition indices=\",random_graph_preprocessing.partition_index)\n",
    "print(\"tv + partition index hot dict=\",random_graph_preprocessing.partition_indices_and_one_hot_tv)\n",
    "print(\"partitioned one-hot encoding, with index 1 if vertex is refined ith time=\",random_graph_preprocessing.partitioned_tv)\n",
    "print(\"index of partitions=\",random_graph_preprocessing.partition_times_hot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee2932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
