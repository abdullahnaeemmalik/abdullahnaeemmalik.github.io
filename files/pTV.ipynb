{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1520285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from time import process_time\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "from torch.sparse import *\n",
    "import collections\n",
    "os.environ['DGLBACKEND'] = 'pytorch'\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ogb.nodeproppred import DglNodePropPredDataset, Evaluator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0929d",
   "metadata": {},
   "source": [
    "This builds the 1-skeleton of a standard simplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516d6fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplexCreator():\n",
    "    \n",
    "    \n",
    "    \"\"\"Create standard simplex\"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, dimension):\n",
    "        self.input_dimension = dimension\n",
    "        self.src=list()\n",
    "        self.dst=list()\n",
    "        for i in range(self.input_dimension+1):\n",
    "            for j in range(self.input_dimension+1):\n",
    "                if (i < j):\n",
    "                    self.src = self.src + [i]\n",
    "                    self.dst = self.dst + [j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed84c77b",
   "metadata": {},
   "source": [
    "The preprocessing step -- this is where we find pseudotop vertices for a given graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a745612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_tensors(tensor_A, tensor_B, batch_size=1000):\n",
    "    \n",
    "    \n",
    "    \"\"\"This finds common edges from two matrices -- effectively computing\n",
    "    the Hadamard product of two matrices with binary entries\n",
    "    However, this process takes way too much space, so won't be used.\"\"\"\n",
    "    \n",
    "    \n",
    "    common_pairs_list = []\n",
    "    print(\"Performing batches of edge comparison\")\n",
    "    for i in tqdm(range(0, tensor_A.size(0), batch_size), position=0, leave=False):\n",
    "        batch_A = tensor_A[i:i+batch_size]\n",
    "        equal_pairs = torch.all(batch_A[:, None, :] == tensor_B[None, :, :], dim=2)\n",
    "        common_pair_indices = torch.nonzero(equal_pairs, as_tuple=False)\n",
    "        common_pairs_list.append(tensor_A[common_pair_indices[:, 0]])\n",
    "    \n",
    "    return torch.cat(common_pairs_list)\n",
    "\n",
    "class PseudoTV():\n",
    "    \n",
    "    \n",
    "    src=list()\n",
    "    dst=list()\n",
    "    empty_graph = dgl.heterograph({('node', 'to', 'node'): (src, dst)})\n",
    "    dimension = int\n",
    "\n",
    "    assert isinstance(empty_graph, dgl.DGLHeteroGraph), \\\n",
    "        'Keyword argument \\\"graph\\\" of AdjGraph\\'s init methodmust be a dgl.DGLHeteroGraph.'\n",
    "\n",
    "    def __init__(\n",
    "        self, file_path, graph=empty_graph,dimension=dimension):\n",
    "        self.seed_graph = graph\n",
    "        self.srcs_and_dsts = self.seed_graph.edges()   \n",
    "        \n",
    "        \"\"\"Create dictionary with dimension (keys) and place list of nodes with \n",
    "        all possible top dimensions (values)\"\"\"\n",
    "        self.top_vertices_dictionary = collections.defaultdict(list)\n",
    "        self.top_vertices_dictionary[0]=[int(x) for x in self.seed_graph.nodes()]\n",
    "        self.top_vertices_dictionary[1]=[int(x) for x in torch.unique(self.srcs_and_dsts[1])]\n",
    "        \"\"\" This dict has keys = dimensions d and values = dictionary. This \n",
    "        subdictionary has keys = nodes and values =\n",
    "        vertices that make the node a top-vertex of dimension d\"\"\"\n",
    "\n",
    "        print(\"Finished adding 0-top vertices and 1-top vertices in main dictionary\")\n",
    "        \n",
    "        \"\"\"compute all in_degrees. These are needed for the algorithm later on \n",
    "        when the loop runs\"\"\"\n",
    "        self.in_degrees = self.seed_graph.in_degrees()\n",
    "        self.out_degrees = self.seed_graph.out_degrees()\n",
    "        self.dimension = dimension\n",
    "        self.maximum_dimension = int(torch.max(self.in_degrees))\n",
    "        \n",
    "        self.file_path=file_path\n",
    "        \n",
    "        \"\"\"Create dictionary with dimension (keys) and list of nodes with maximum\n",
    "        dimension corresponding to key (values)\"\"\"\n",
    "        self.pseudo_top_vertices_dict = collections.defaultdict(list)\n",
    "        for v in self.seed_graph.nodes():\n",
    "            \"\"\"Add all zero-dimension vertices, and for now keep rest as 1-dimension vertices.\n",
    "            The vertices moved to different keys as more simplices are identified\"\"\"\n",
    "            if self.in_degrees[v] == 0:\n",
    "                self.pseudo_top_vertices_dict[0] = self.pseudo_top_vertices_dict[0] + [int(v)]\n",
    "            else:\n",
    "                self.pseudo_top_vertices_dict[1] = self.pseudo_top_vertices_dict[1] + [int(v)]\n",
    "                \n",
    "        print(\"Finished adding 0-pseudo top vertices and 1-pseudo top vertices\")\n",
    "        \n",
    "        \"\"\"Create empty dictionary as above, but this time will have sets \n",
    "        (value) for each dimension (key)\"\"\"        \n",
    "        self.pseudo_top_vertices_dict_of_sets = dict()\n",
    "        \n",
    "        \"\"\"Same values as above, but keys are addresses of the sets\"\"\" \n",
    "        self._sets = dict()\n",
    "        \n",
    "        \"\"\" Creates dictionary from above sets that has \n",
    "        representatives (keys) and sets (values)\"\"\"\n",
    "        self._partition = dict()\n",
    "\n",
    "        \"\"\" Needed for refinement. Finds vertices in same class for \n",
    "        each iteration of refinement\"\"\"\n",
    "        self.refined_partition = dict()\n",
    "        \n",
    "        \"\"\" Number of refinements done after TV identification\n",
    "        for each vertex. Needed to find final partition index\"\"\"\n",
    "        self.partition_number = 0\n",
    "        \n",
    "        \"\"\" Dictionary with keys = nodes as values = partition \n",
    "        index of that node. Currently at zero, since\n",
    "        no refinement is done, yet! The values for each key \n",
    "        is supposed to be the partition_number\"\"\"\n",
    "        self.partition_index = {int_node: 0 for int_node in self.top_vertices_dictionary[0]}\n",
    "        \n",
    "        \"\"\"Create dictionary with keys = nodes and values = \n",
    "        one-hot tensor of top-vertex and parition index,\n",
    "        both concatenated\"\"\"\n",
    "        self.partition_indices_and_one_hot_tv = {int_node: [] for int_node in self.top_vertices_dictionary[0]}\n",
    "        \n",
    "        \"\"\" Create dictionary with keys = nodes and values = \n",
    "        one hot encoding of pseudo top dimension. That is\n",
    "        ith index 1 if top maximum dimension is equal to \n",
    "        index and zero otherwise \"\"\"\n",
    "        self.one_hot_dict = collections.defaultdict(list)\n",
    "\n",
    "        \"\"\" Create dictionary with keys = nodes. The values \n",
    "        for the dictionary \n",
    "        are tensors which are multi hot encoding with ith \n",
    "        index 1 for i-top dimension and zero otherwise \"\"\"\n",
    "        self.multi_hot_tv_dict = collections.defaultdict(list)\n",
    "        \n",
    "        \"\"\" Create dictionary with keys = nodes and values = \n",
    "        one hot encoding with ith index 1 if vertex is refined i\n",
    "        times and zero otherwise. This is an index of number of \n",
    "        times a vertex has been partitioned. \"\"\"\n",
    "        self.partition_times_hot_dict = collections.defaultdict(list)\n",
    "        \n",
    "        \"\"\" Create dictionary with keys = nodes and values = \n",
    "        one hot encoding with ith index 1 if vertex is in the\n",
    "        ith partition and zero otherwise. This captures the \n",
    "        number of partitions after refinement. \"\"\"\n",
    "        self.partitioned_tv = collections.defaultdict(list)\n",
    "        \n",
    "        \"\"\" Create a dictionary of vectors R(v) for each vertex v. \n",
    "        This gets updated at each refinement process. \n",
    "        The values are stored since then the algorithm wouldn't \n",
    "        have to create the vector each time its needed\"\"\"\n",
    "        self.refinement_vectors_dict = collections.defaultdict(list)\n",
    "        \n",
    "        \"\"\" Boolean expression to see if refinement needs to proceed\"\"\"\n",
    "        self.partition_proceeds = True\n",
    "        \n",
    "    def kill_diag_make_binary(self,matrix):\n",
    "        \n",
    "        \n",
    "        diagonal_mask = (matrix._indices()[0] == matrix._indices()[1])\n",
    "        off_diagonal_mask = ~diagonal_mask\n",
    "        #set all zero values to one where the edge is not a loop\n",
    "        matrix._values()[off_diagonal_mask] = 1.0\n",
    "        #create a new sparse matrix with diagonal elements killed off\n",
    "        new_indices = matrix._indices()[:, off_diagonal_mask]\n",
    "        #only use original nonzero values (which were later changed to 1)\n",
    "        new_values = matrix._values()[off_diagonal_mask]\n",
    "        new_size = matrix.size()\n",
    "        return torch.sparse_coo_tensor(indices=new_indices, values=new_values, size=new_size)\n",
    "    \n",
    "    def hadamard_prod(self, matrix1, matrix2):\n",
    "        \n",
    "        \n",
    "        false_hadamard_product = matrix1 * matrix2\n",
    "        false_hadamard_product = false_hadamard_product.coalesce()\n",
    "        non_zero_mask = false_hadamard_product._values().nonzero().squeeze()\n",
    "        non_zero_values = false_hadamard_product._values()[non_zero_mask]\n",
    "        non_zero_indices = false_hadamard_product.indices()[:, non_zero_mask]\n",
    "        hadamard_product = torch.sparse_coo_tensor(indices=non_zero_indices,\n",
    "                                                   values=non_zero_values,\n",
    "                                                   size=false_hadamard_product.size())\n",
    "        \"\"\"Alternate approach, and the correct one without bugs.\n",
    "        Isn't used.\"\"\"\n",
    "        #edges_1 = matrix1._indices().transpose(0, 1)\n",
    "        #edges_2 = matrix2._indices().transpose(0, 1)\n",
    "        #edges = find_common_tensors(edges_1, edges_2)\n",
    "        #adj_size =  len(self.seed_graph.nodes())\n",
    "        #indices = edges.t().long()\n",
    "        #values = torch.ones(edges.shape[0], dtype=torch.int64)\n",
    "        #hadamard_product = torch.sparse_coo_tensor(indices=indices, \n",
    "                                                   #values=values, size=torch.Size([adj_size, adj_size]))\n",
    "        return hadamard_product\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        \n",
    "        \"\"\"Return the size of the partition.\"\"\"\n",
    "        \n",
    "        \n",
    "        return len(self._sets)\n",
    "\n",
    "    def out_nodes_as_int(self, vertex):\n",
    "        \n",
    "        \n",
    "        \"\"\"convert successors to a list with integer node values\"\"\"\n",
    "        \n",
    "        \n",
    "        neighbors = [int(v) for v in list(self.seed_graph.successors(vertex))]\n",
    "        if int(vertex) in neighbors:\n",
    "            neighbors.remove(int(vertex))\n",
    "        return neighbors\n",
    "\n",
    "    def in_nodes_as_int(self, vertex):\n",
    "        \n",
    "        \n",
    "        \"\"\"convert predecessors to a list with integer node values\"\"\"\n",
    "        \n",
    "        \n",
    "        neighbors = [int(v) for v in list(self.seed_graph.predecessors(vertex))]\n",
    "        if int(vertex) in neighbors:\n",
    "            neighbors.remove(int(vertex))\n",
    "        return neighbors       \n",
    "    \n",
    "    def inductive_connecting(self):\n",
    "        \n",
    "        \n",
    "        #Case for k=2 doesn't need to search for intersections:\n",
    "        print(\"Computing matrices of dimension 2\")\n",
    "        no_diag_binary = torch.sparse.mm(self.seed_graph.adj_external(),\n",
    "                                         self.seed_graph.adj_external())\n",
    "        no_diag_binary = self.kill_diag_make_binary(no_diag_binary) \n",
    "        self.hadamard_product_next = self.hadamard_prod(\n",
    "            self.seed_graph.adj_external(),no_diag_binary)\n",
    "        print(\"Adding 2-vertices to the dictionary\")\n",
    "        row_indices_analyzed_node, col_indices_analyzed_node = self.hadamard_product_next._indices()\n",
    "        for vertex in col_indices_analyzed_node.unique():\n",
    "            self.top_vertices_dictionary[2] = self.top_vertices_dictionary[2] + [int(vertex)]\n",
    "            self.pseudo_top_vertices_dict[2] = self.pseudo_top_vertices_dict[2] + [int(vertex)]\n",
    "            if vertex in self.pseudo_top_vertices_dict[1]:\n",
    "                self.pseudo_top_vertices_dict[1].remove(int(vertex))\n",
    "        print(\"Finished adding vertices of dimension 2\")\n",
    "        #remove these variables from memory\n",
    "        row_indices_analyzed_node = None\n",
    "        col_indices_analyzed_node = None\n",
    "        \n",
    "        for k in tqdm(range(3,self.dimension+1), position=0, leave=False):\n",
    "            no_diag_binary = torch.sparse.mm(self.seed_graph.adj_external(),no_diag_binary)\n",
    "            no_diag_binary = self.kill_diag_make_binary(no_diag_binary)\n",
    "            #compute A⚬A^2⚬..⚬A^k where ⚬ denotes Hadamard product\n",
    "            self.hadamard_product_prev  = self.hadamard_product_next.clone().detach()\n",
    "            self.hadamard_product_next = self.hadamard_prod(\n",
    "                self.hadamard_product_prev,no_diag_binary)\n",
    "            print(\"Finished computing matrices for dim\",k)\n",
    "            \n",
    "            for v in tqdm(self.seed_graph.nodes(), position=0, leave=False):\n",
    "                if v in self.top_vertices_dictionary[k-1]:\n",
    "                    if self.in_degrees[v] < k:\n",
    "                        continue\n",
    "                    else:\n",
    "                        current_in_neighbors_of_node_analyzed = self.in_nodes_as_int(v)\n",
    "                        for u in tqdm(self.top_vertices_dictionary[k-1], position=0, leave=False):\n",
    "                            if u == v:\n",
    "                                continue\n",
    "                            if u not in current_in_neighbors_of_node_analyzed:\n",
    "                                continue\n",
    "                            else:\n",
    "                                A = self.hadamard_product_prev\n",
    "                                B = self.hadamard_product_next\n",
    "                                A = A.coalesce()\n",
    "                                B = B.coalesce()\n",
    "                                A_row, A_col = A.indices()\n",
    "                                B_row, B_col = B.indices()\n",
    "                                if len(B_col) == 0:\n",
    "                                    print(\"We have reached maximum dimension, and that is\",k-1)\n",
    "                                    self.maximum_dimension = k-1\n",
    "                                    self.top_vertices_dictionary.pop(k, None)\n",
    "                                    return\n",
    "                                A_u_rows = A_row[torch.where(A_col.eq(u))]\n",
    "                                B_v_rows = B_row[torch.where(B_col.eq(v))]\n",
    "                                common_rows = np.intersect1d(A_u_rows, B_v_rows)\n",
    "                                #these are all the vertices for which there is a path with unique vertices\n",
    "                                #of length 1, length 2, ..., length k-1 to both u and v\n",
    "                                intersection_criterion = False\n",
    "                                for i in common_rows:\n",
    "                                    intersection = set(self.out_nodes_as_int(i)).intersection(\n",
    "                                        set(self.in_nodes_as_int(u))).intersection(set(current_in_neighbors_of_node_analyzed))\n",
    "                                    if len(intersection) > k-3:\n",
    "                                        intersection_criterion = True\n",
    "                                        break\n",
    "                                if not(intersection_criterion):\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    self.top_vertices_dictionary[k] = self.top_vertices_dictionary[k] + [int(v)]\n",
    "                                    self.pseudo_top_vertices_dict[k] = self.pseudo_top_vertices_dict[k] + [int(v)]\n",
    "                                    if v in self.pseudo_top_vertices_dict[k-1]:\n",
    "                                        self.pseudo_top_vertices_dict[k-1].remove(int(v))\n",
    "                                        \"\"\"a new top vertex v is found, so we can move out of the neighborhood of v\"\"\"\n",
    "                                    break\n",
    "                        \n",
    "            if len(self.top_vertices_dictionary[k]) == 0:\n",
    "                print(\"We have reached maximum dimension, and that is\",k-1)\n",
    "                self.maximum_dimension = k-1\n",
    "                self.dimension = k-1\n",
    "                self.top_vertices_dictionary.pop(k, None)\n",
    "                break\n",
    "        print(\"Now creating other initial dictionaries\")\n",
    "        self.pseudo_top_vertices_dict_of_sets = {key:set(self.pseudo_top_vertices_dict[key]) \n",
    "                                                for key in range(0,self.dimension+1)}\n",
    "        \n",
    "        self._sets = {id(self.pseudo_top_vertices_dict_of_sets[key]):self.pseudo_top_vertices_dict_of_sets[key]\n",
    "                                 for key in self.pseudo_top_vertices_dict_of_sets.keys()}\n",
    "        self._partition = {x:self.pseudo_top_vertices_dict_of_sets[key] \n",
    "                              for key in self.pseudo_top_vertices_dict_of_sets.keys() \n",
    "                           for x in self.pseudo_top_vertices_dict_of_sets[key]}\n",
    "        print(\"Finished creating initial partition\")\n",
    "        print(\"Serializing dictionaries..\")\n",
    "        data_to_save = {\"pseudo_top_vertices_dict\": {str(key): value \n",
    "                                                     for key, value in self.pseudo_top_vertices_dict.items()}}\n",
    "        data_to_save = {\n",
    "            \"pseudo_top_vertices_dict\": {str(key): value \n",
    "                               for key, value in self.pseudo_top_vertices_dict.items()},\n",
    "            \"top_vertices_dict\": {str(key): value \n",
    "                               for key, value in self.top_vertices_dictionary.items()}}\n",
    "        \n",
    "        \n",
    "        with open('pseudo_tv.json', \"w\") as file:\n",
    "            json.dump(data_to_save, file)\n",
    "        print(\"Dictionary of pseudo top vertices saved as a JSON file\")\n",
    "    \n",
    "    def partition_vector(self,vertex):\n",
    "        \n",
    "        \n",
    "        \"\"\" Create vector R(v) for vertex v \"\"\" \n",
    "        \n",
    "        \n",
    "        vector = [self._partition[vertex]]\n",
    "        for v in torch.sort(self.seed_graph.predecessors(vertex))[0]:\n",
    "            \"\"\" The representatives have to be sorted for a meaningful comparison of vectors \"\"\"\n",
    "            if torch.eq(v,vertex):\n",
    "                pass\n",
    "            else:\n",
    "                vector = vector + [self._partition[int(v)]]\n",
    "        return vector\n",
    "        \n",
    "    def refine(self):\n",
    "        \n",
    "        \n",
    "        \"\"\"Original idea for refinement algorithm by David Eppstein. \n",
    "        Refine each set A in the partition to the two sets\n",
    "        A & S, A - S.  Also produces output (A & S, A - S)\n",
    "        for each changed set.  Within each pair, A & S will be\n",
    "        a newly created set, while A - S will be a modified\n",
    "        version of an existing set in the partition.\n",
    "        \n",
    "        Hit here is a dictionary with keys = addresses for \n",
    "        original partitions and values = vertices with common\n",
    "        partition vector\"\"\"\n",
    "        \n",
    "        \n",
    "        hit = self.refined_partition\n",
    "        output = list()\n",
    "        for A,AS in hit.items():\n",
    "            A = self._sets[A]\n",
    "            \"\"\"Check if new partition is not the same as the old partition\"\"\"\n",
    "            if AS != A:\n",
    "                self._sets[id(AS)] = AS\n",
    "                \"\"\" This loop finds elements that are not part of previous partition\"\"\"\n",
    "                for x in AS:\n",
    "                    self._partition[x] = AS\n",
    "                \"\"\"The elements that were not part of the partition are now A\"\"\"\n",
    "                A -= AS\n",
    "                output = output + [set.union(A,AS)]\n",
    "        \"\"\" output here keeps track of those partitions that have been broken down\"\"\"\n",
    "        refined_vertices = set().union(*output)\n",
    "        \"\"\"The partitioning process above, once done, should then \n",
    "        increase the partition_number, if the above\n",
    "        does indeed count as another genuine refinement. If it \n",
    "        does not, then the number of refined_vertices\n",
    "        is zero, and hence should not increase partition number\"\"\"\n",
    "        if len(refined_vertices) == 0:\n",
    "            self.partition_proceeds = False\n",
    "            return\n",
    "        else:\n",
    "            \"\"\"If there is a refinement that takes place, then we \n",
    "            increase the partition number and attach\n",
    "            this as the partition index for each vertex in the \n",
    "            partition_index dictionary\"\"\"\n",
    "            self.partition_number = self.partition_number+1\n",
    "            for v in refined_vertices:\n",
    "                self.partition_index[v] = self.partition_number\n",
    "\n",
    "    def refinement(self):\n",
    "        \n",
    "        \n",
    "        \"\"\"Keep on refining the partitions until the partition stabilizes\"\"\"\n",
    "        \n",
    "        \n",
    "        while self.partition_proceeds:\n",
    "            print(\"Refining..\")\n",
    "            \"\"\"finds vertices u and v such that R(u) = R(v) and make refined partitions here\"\"\"\n",
    "            common_vertices = dict()\n",
    "            for node in tqdm(self.seed_graph.nodes(), position=0, leave=False):\n",
    "                self.refinement_vectors_dict[int(node)] = self.partition_vector(int(node))\n",
    "            print(\"Finished creating list of partition vectors for\", \n",
    "                  \"partition iteration number\",self.partition_number)\n",
    "            print(\"Finding vertices with common partition vectors..\")\n",
    "            \"\"\"This step could be modified for optimization. It needlessly also checks\n",
    "            for partion vectors of vertices that have not been partitioned the first time\"\"\"\n",
    "            for v,u in tqdm(itertools.combinations(\n",
    "                self.seed_graph.nodes(),2), position=0, leave=False):\n",
    "                if self.refinement_vectors_dict[int(v)] == self.refinement_vectors_dict[int(u)]:\n",
    "                    Au = self._partition[int(u)]\n",
    "                    common_vertices.setdefault(id(Au),set()).update([int(u),int(v)])\n",
    "            self.refined_partition=common_vertices  \n",
    "            self.refine()\n",
    "        print(\"Refinement process finished\")\n",
    "\n",
    "    def add_vertex_features(self):\n",
    "        \n",
    "        \n",
    "        \"\"\"First, we start with refinement until the partition stabilizes\"\"\"\n",
    "        \n",
    "        \n",
    "        sets_for_partition_as_list = list(self._sets.values())\n",
    "        print(\"Adding in node features..\")\n",
    "        for node in tqdm(self.seed_graph.nodes(), position=0, leave=False):\n",
    "            \"\"\" Fills in the following dictionaries\n",
    "            1) multi_hot_tv_dict\n",
    "            2) one_hot_dict\n",
    "            3) partition_indices_and_one_hot_tv\n",
    "            4) partition_times_hot_dict\n",
    "            5) partitioned_tv\n",
    "            by filling in each key (node)\"\"\"\n",
    "            pihvector = [0] * (self.partition_number+1)\n",
    "            mhvector = [0] * (self.dimension+1)\n",
    "            ohvector = [0] * (self.dimension+1)\n",
    "            ptvvector = [0] * (len(self._sets))\n",
    "            node = int(node)\n",
    "            for dim in range(0,self.dimension+1):\n",
    "                if node in self.top_vertices_dictionary[dim]:\n",
    "                    mhvector[dim] = 1\n",
    "                if node in self.pseudo_top_vertices_dict[dim]:\n",
    "                    ohvector[dim] = 1\n",
    "            pihvector[self.partition_index[node]] = 1\n",
    "            self.multi_hot_tv_dict[node] = mhvector\n",
    "            self.one_hot_dict[node] = ohvector\n",
    "            self.partition_times_hot_dict[node] = pihvector\n",
    "            temp_vector = self.one_hot_dict[node] + self.partition_times_hot_dict[node]\n",
    "            self.partition_indices_and_one_hot_tv[node] = temp_vector\n",
    "            index = sets_for_partition_as_list.index(self._partition[node])\n",
    "            ptvvector[index] = 1\n",
    "            self.partitioned_tv[node] = ptvvector\n",
    "        print(\"Vertex features added!\")\n",
    "          \n",
    "    def save_dicts(self):\n",
    "        \n",
    "        \n",
    "        print(\"Serializing dictionaries for vertex features..\")\n",
    "        data_to_save = {\n",
    "            \"partitioned_tv\": {str(key): value \n",
    "                               for key, value in self.partitioned_tv.items()},\n",
    "            \"partition_indices_and_one_hot_tv\": {str(key): value \n",
    "                                                 for key, value in self.partition_indices_and_one_hot_tv.items()},\n",
    "            \"multi_hot_tv_dict\": {str(key): value \n",
    "                                  for key, value in self.multi_hot_tv_dict.items()},\n",
    "            \"partition_times_hot_dict\" : {str(key): value \n",
    "                                          for key, value in self.partition_times_hot_dict.items()}\n",
    "        }\n",
    "        \n",
    "        with open(self.file_path, \"w\") as file:\n",
    "            json.dump(data_to_save, file)\n",
    "        print(\"Dictionaries saved as a JSON file\")\n",
    "        \n",
    "    def load_dicts(self):\n",
    "        \n",
    "        \n",
    "        with open(self.file_path, \"r\") as file:\n",
    "            loaded_data = json.load(file)\n",
    "        \n",
    "        self.partitioned_tv = {int(key): torch.tensor(value)\n",
    "                               for key, value in loaded_data[\"partitioned_tv\"].items()}\n",
    "        \n",
    "        self.partition_indices_and_one_hot_tv = {int(key): torch.tensor(value)\n",
    "                                                 for key, value in loaded_data[\"partition_indices_and_one_hot_tv\"].items()}\n",
    "        \n",
    "        self.multi_hot_tv_dict = {int(key): torch.tensor(value)\n",
    "                                  for key, value in loaded_data[\"multi_hot_tv_dict\"].items()}\n",
    "        \n",
    "        self.partition_times_hot_dict = {int(key): torch.tensor(value)\n",
    "                                         for key, value in loaded_data[\"partition_times_hot_dict\"].items()}\n",
    "        \n",
    "    def load_ptv_dict(self):\n",
    "        \n",
    "        \n",
    "        with open('pseudo_tv.json', \"r\") as file:\n",
    "            loaded_data = json.load(file)        \n",
    "        self.pseudo_top_vertices_dict = {int(key): list(value) \n",
    "                                         for key, value in loaded_data[\"pseudo_top_vertices_dict\"].items()}\n",
    "        self.top_vertices_dictionary = {int(key): list(value) \n",
    "                                         for key, value in loaded_data[\"top_vertices_dict\"].items()}\n",
    "        print(\"Now creating other initial dictionaries\")\n",
    "        self.pseudo_top_vertices_dict_of_sets = {key:set(self.pseudo_top_vertices_dict[key]) \n",
    "                                                for key in range(0,self.dimension+1)}\n",
    "        self._sets = {id(self.pseudo_top_vertices_dict_of_sets[key]):self.pseudo_top_vertices_dict_of_sets[key]\n",
    "                                 for key in self.pseudo_top_vertices_dict_of_sets.keys()}\n",
    "        self._partition = {x:self.pseudo_top_vertices_dict_of_sets[key] for key in self.pseudo_top_vertices_dict_of_sets.keys() for x in self.pseudo_top_vertices_dict_of_sets[key]}\n",
    "        print(\"Finished creating initial partition\")\n",
    "\n",
    "    def make_plots(self, dict_name):\n",
    "        \n",
    "        \n",
    "        d = self.dimension    \n",
    "        # x axis values \n",
    "        x = range(0, d+1)\n",
    "        print(\"d=\",d)\n",
    "        # corresponding y axis values\n",
    "        if dict_name not in ['partitioned_tv','tv']:\n",
    "            raise ValueError(\"Invalid dictionary name. Must be either partitioned_tv or tv\")\n",
    "        y = list()\n",
    "        if dict_name == 'partitioned_tv':\n",
    "            for key in self._sets.keys():\n",
    "                y.append(len(self._sets[key]))\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.bar(x, y, color='blue') \n",
    "            plt.xlabel('dimension') \n",
    "            plt.ylabel('Number of elements in partition')\n",
    "            plt.title('representative') \n",
    "            plt.xticks(x)\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.savefig('partitionedtv_plot.png')\n",
    "        if dict_name == 'tv':\n",
    "            for key in self.top_vertices_dictionary.keys():\n",
    "                y.append(len(self.top_vertices_dictionary[key]))\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.bar(x, y, color='blue') \n",
    "            plt.xlabel('dimension') \n",
    "            plt.ylabel('Number of partitioned top vertices')\n",
    "            plt.title('dimension distribution') \n",
    "            plt.xticks(x)\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.savefig('partitionedtv_plot.png')          \n",
    "\n",
    "class PartitionError(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3de2fe38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished adding 0-top vertices and 1-top vertices in main dictionary\n",
      "Finished adding 0-pseudo top vertices and 1-pseudo top vertices\n",
      "Computing matrices of dimension 2\n",
      "Adding 2-vertices to the dictionary\n",
      "Finished adding vertices of dimension 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished computing matrices for dim 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished computing matrices for dim 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now creating other initial dictionaries\n",
      "Finished creating initial partition\n",
      "Serializing dictionaries..\n",
      "Dictionary of pseudo top vertices saved as a JSON file\n",
      "Top vertices dictionary= defaultdict(<class 'list'>, {0: [0, 1, 2, 3, 4, 5], 1: [1, 2, 3, 4, 5], 2: [2, 3, 4, 5], 3: [3, 4, 5], 4: [4, 5]})\n",
      "Partition by dimension= defaultdict(<class 'list'>, {0: [0], 1: [1], 2: [2], 3: [3], 4: [4, 5]})\n",
      "top vertex dictionary= defaultdict(<class 'list'>, {0: [0, 1, 2, 3, 4, 5], 1: [1, 2, 3, 4, 5], 2: [2, 3, 4, 5], 3: [3, 4, 5], 4: [4, 5]})\n",
      "Partitions before refinement= {0: {0}, 1: {1}, 2: {2}, 3: {3}, 4: {4, 5}, 5: {4, 5}}\n",
      "Refining..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating list of partition vectors for partition iteration number 0\n",
      "Finding vertices with common partition vectors..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refinement process finished\n",
      "Adding in node features..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex features added!\n",
      "Partitions after refinement= {0: {0}, 1: {1}, 2: {2}, 3: {3}, 4: {4, 5}, 5: {4, 5}}\n",
      "One hot encoding of pure tv= defaultdict(<class 'list'>, {0: [1, 0, 0, 0, 0], 1: [0, 1, 0, 0, 0], 2: [0, 0, 1, 0, 0], 3: [0, 0, 0, 1, 0], 4: [0, 0, 0, 0, 1], 5: [0, 0, 0, 0, 1]})\n",
      "multi hot encoding of all dimensions for tv= defaultdict(<class 'list'>, {0: [1, 0, 0, 0, 0], 1: [1, 1, 0, 0, 0], 2: [1, 1, 1, 0, 0], 3: [1, 1, 1, 1, 0], 4: [1, 1, 1, 1, 1], 5: [1, 1, 1, 1, 1]})\n",
      "partition indices= {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0}\n",
      "tv + partition index hot dict= {0: [1, 0, 0, 0, 0, 1], 1: [0, 1, 0, 0, 0, 1], 2: [0, 0, 1, 0, 0, 1], 3: [0, 0, 0, 1, 0, 1], 4: [0, 0, 0, 0, 1, 1], 5: [0, 0, 0, 0, 1, 1]}\n",
      "partitioned one-hot encoding, with index 1 if vertex is refined ith time= defaultdict(<class 'list'>, {0: [1, 0, 0, 0, 0], 1: [0, 1, 0, 0, 0], 2: [0, 0, 1, 0, 0], 3: [0, 0, 0, 1, 0], 4: [0, 0, 0, 0, 1], 5: [0, 0, 0, 0, 1]})\n",
      "index of partitions= defaultdict(<class 'list'>, {0: [1], 1: [1], 2: [1], 3: [1], 4: [1], 5: [1]})\n",
      "d= 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Code testing\"\"\"\n",
    "K_10 = dgl.heterograph({('paper', 'cites', 'paper'): (SimplexCreator(dimension=5).src, SimplexCreator(dimension=5).dst)})\n",
    "filepath = 'K_10'\n",
    "K_10_preprocessing = PseudoTV(filepath,graph=K_10,dimension=4)\n",
    "K_10_preprocessing.inductive_connecting()\n",
    "#K_10_preprocessing.load_ptv_dict()\n",
    "print(\"Top vertices dictionary=\",K_10_preprocessing.top_vertices_dictionary)\n",
    "print(\"Partition by dimension=\",K_10_preprocessing.pseudo_top_vertices_dict)\n",
    "print(\"top vertex dictionary=\",K_10_preprocessing.top_vertices_dictionary)\n",
    "print(\"Partitions before refinement=\",K_10_preprocessing._partition)\n",
    "K_10_preprocessing.refinement()\n",
    "K_10_preprocessing.add_vertex_features()\n",
    "print(\"Partitions after refinement=\",K_10_preprocessing._partition)\n",
    "print(\"One hot encoding of pure tv=\",K_10_preprocessing.one_hot_dict)\n",
    "print(\"multi hot encoding of all dimensions for tv=\",K_10_preprocessing.multi_hot_tv_dict)\n",
    "print(\"partition indices=\",K_10_preprocessing.partition_index)\n",
    "print(\"tv + partition index hot dict=\",K_10_preprocessing.partition_indices_and_one_hot_tv)\n",
    "print(\"partitioned one-hot encoding, with index 1 if vertex is refined ith time=\",K_10_preprocessing.partitioned_tv)\n",
    "print(\"index of partitions=\",K_10_preprocessing.partition_times_hot_dict)\n",
    "#K_10_preprocessing.make_plots('partitioned_tv')\n",
    "K_10_preprocessing.make_plots('tv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0935789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Code testing\"\"\"\n",
    "src_bell = [0,0,0,1,1,2] + [1] + [4,4,4,5,5,6]\n",
    "dst_bell = [1,2,3,2,3,3] + [4] + [5,6,7,6,7,7]\n",
    "bell = dgl.heterograph({('paper', 'cites', 'paper'): (src_bell, dst_bell)})\n",
    "filepath = 'bell'\n",
    "bell_preprocessing = PseudoTV(filepath,graph=bell,dimension=30)\n",
    "bell_preprocessing.inductive_connecting()\n",
    "print(\"Top vertices dictionary=\",bell_preprocessing.top_vertices_dictionary)\n",
    "print(\"Partition by dimension=\",bell_preprocessing.pseudo_top_vertices_dict)\n",
    "print(\"top vertex dictionary=\",bell_preprocessing.top_vertices_dictionary)\n",
    "print(\"Partitions before refinement=\",bell_preprocessing._partition)\n",
    "bell_preprocessing.refinement()\n",
    "bell_preprocessing.add_vertex_features()\n",
    "print(\"Partitions after refinement=\",bell_preprocessing._partition)\n",
    "print(\"One hot encoding of pure tv=\",bell_preprocessing.one_hot_dict)\n",
    "print(\"multi hot encoding of all dimensions for tv=\",bell_preprocessing.multi_hot_tv_dict)\n",
    "print(\"partition indices=\",bell_preprocessing.partition_index)\n",
    "print(\"tv + partition index hot dict=\",bell_preprocessing.partition_indices_and_one_hot_tv)\n",
    "print(\"partitioned one-hot encoding, with index 1 if vertex is refined ith time=\",bell_preprocessing.partitioned_tv)\n",
    "print(\"index of partitions=\",bell_preprocessing.partition_times_hot_dict)\n",
    "#bell_preprocessing.save_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4a1ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_random_graph(num_nodes):\n",
    "    src_edges =[]\n",
    "    dst_edges = []\n",
    "    edges = []\n",
    "    for i in range(2*num_nodes):\n",
    "        src_edges.append(random.randint(0,num_nodes))\n",
    "        dst_edges.append(random.randint(0,num_nodes))\n",
    "        edges.append((src_edges[i],dst_edges[i]))\n",
    "    graph = dgl.heterograph({('paper', 'cites', 'paper'): (src_edges, dst_edges)})\n",
    "    return graph, edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120396ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just to visualize the random graph.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "dgl_G, edges = generate_random_graph(30)\n",
    "print(edges)\n",
    "nx_G = nx.DiGraph()\n",
    "nx_G.add_edges_from(edges)\n",
    "options = {\n",
    "    'node_color': 'black',\n",
    "    'node_size': 20,\n",
    "    'width': 1,\n",
    "}\n",
    "pos = nx.spring_layout(nx_G, seed=42)\n",
    "pos = nx.planar_layout(nx_G)\n",
    "nx.draw_networkx(nx_G, pos, with_labels=True, node_color='lightblue', node_size=200, font_size=10, font_color='black', arrows=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dbe128",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath='randomgraph'\n",
    "random_graph_preprocessing = PseudoTV(filepath,graph=dgl_G,dimension=30)\n",
    "random_graph_preprocessing.inductive_connecting()\n",
    "print(\"Top vertices dictionary=\",random_graph_preprocessing.top_vertices_dictionary)\n",
    "print(\"Partition by dimension=\",random_graph_preprocessing.pseudo_top_vertices_dict)\n",
    "print(\"top vertex dictionary=\",random_graph_preprocessing.top_vertices_dictionary)\n",
    "print(\"Partitions before refinement=\",random_graph_preprocessing._partition)\n",
    "random_graph_preprocessing.refinement()\n",
    "random_graph_preprocessing.add_vertex_features()\n",
    "print(\"Partitions after refinement=\",random_graph_preprocessing._partition)\n",
    "print(\"One hot encoding of pure tv=\",random_graph_preprocessing.one_hot_dict)\n",
    "print(\"multi hot encoding of all dimensions for tv=\",random_graph_preprocessing.multi_hot_tv_dict)\n",
    "print(\"partition indices=\",random_graph_preprocessing.partition_index)\n",
    "print(\"tv + partition index hot dict=\",random_graph_preprocessing.partition_indices_and_one_hot_tv)\n",
    "print(\"partitioned one-hot encoding, with index 1 if vertex is refined ith time=\",random_graph_preprocessing.partitioned_tv)\n",
    "print(\"index of partitions=\",random_graph_preprocessing.partition_times_hot_dict)\n",
    "#random_graph_preprocessing.save_dicts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c9d5ca",
   "metadata": {},
   "source": [
    "Using the graph convolution network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f79b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, graph, input_layer:int, hidden_layers:int, output_layer:int, \n",
    "                 num_layers:int, dropout):\n",
    "        \"\"\"\n",
    "        Defines the architecture of your model.  All layers with learnable parameters should\n",
    "        be created in this method.  The `forward` method will define how to use the \n",
    "        layers created here.\n",
    "        \n",
    "        You will also need to add arguments to the `__init__` method that you need to \n",
    "        create your layers.  For example, you might want to include a `num_layers` argument\n",
    "        so that you can dynamically change the number of layers, and a `dropout` argument \n",
    "        so that this is easy to change.\n",
    "        \"\"\"\n",
    "        self.input_layer   = input_layer\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_layer  = output_layer\n",
    "        self.num_layers    = num_layers\n",
    "        self.dropout       = dropout\n",
    "        self.graph         = graph\n",
    "        super(GCN, self).__init__()\n",
    "        self.convs         = nn.ModuleList()\n",
    "        self.bns           = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_layers))\n",
    "        self.convs.append(GraphConv(input_layer, hidden_layers, norm='both', weight=True, bias=True))\n",
    "        \n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GraphConv(hidden_layers, hidden_layers,\n",
    "                                           norm='both', weight=True, bias=True))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_layers))\n",
    "        self.convs.append(GraphConv(hidden_layers, output_layer, \n",
    "                                    norm='both', weight=True, bias=True))\n",
    "\n",
    "    def forward(self, graph, input_features):\n",
    "        \"\"\"\n",
    "        The forward pass of the model, which applies all of the layers\n",
    "        to a given graph and set of node features\n",
    "        \n",
    "        Args:\n",
    "            g (DGLGraph): the graph used for Graph Convolutions\n",
    "            in_feat (Tensor): the node features\n",
    "        \"\"\"\n",
    "        # Stack of model components to compute the forward pass \n",
    "        for conv in self.convs[:-1]:\n",
    "            input_features = conv(graph, input_features)\n",
    "            input_features = F.relu(input_features)\n",
    "            input_features = F.dropout(input_features, p=self.dropout, training=self.training)\n",
    "        input_features = self.convs[-1](graph, input_features)\n",
    "        return input_features.log_softmax(dim=-1)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99eee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Helper Functions\"\"\"\n",
    "\n",
    "def plot_losses(train_losses, val_losses, modelname, log=False):\n",
    "    \"\"\"\n",
    "    Plots train/validation loss curves vs training epoch\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(train_losses, label='Train')\n",
    "    ax.plot(val_losses, label='Val')\n",
    "    ax.set(xlabel='Epoch', ylabel='CrossEnt')\n",
    "    if log:\n",
    "        ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "    filename = f\"plot_losses_{timestamp,modelname}.png\"\n",
    "    picklename_t = f\"train_losses_{timestamp,modelname}.pkl\"\n",
    "    picklename_l = f\"train_losses_{timestamp,modelname}.pkl\"\n",
    "    plt.savefig(filename)\n",
    "    \n",
    "    with open(picklename_t, 'wb') as file:\n",
    "        pickle.dump(train_losses, file)\n",
    "    \n",
    "    with open(picklename_l, 'wb') as file:\n",
    "        pickle.dump(val_losses, file)\n",
    "        \n",
    "    \n",
    "def train(graph, labels, split_idx, model, epochs, evaluator, \n",
    "          device, save_path, loss_fn=F.cross_entropy, lr=0.01, es_criteria=5, verbose=False):\n",
    "    \"\"\"\n",
    "    A standard interface for model training.  Should be no reason to change this unless you \n",
    "    want to add improvements (e.g., learning rate scheduler).\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "    train_losses = list()\n",
    "    val_losses = list()\n",
    "\n",
    "    features = graph.ndata['feat']\n",
    "    \n",
    "    train_mask = split_idx['train'].to(device)\n",
    "    val_mask = split_idx['valid'].to(device)\n",
    "    test_mask = split_idx['test'].to(device)\n",
    "    es_iters = 0\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        \n",
    "        train_loss, val_loss = train_step(\n",
    "            model, graph, features, labels, train_mask, val_mask, optimizer, loss_fn\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc, val_acc, test_acc = test(model, graph, labels, split_idx, evaluator)\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            es_iters = 0\n",
    "        else:\n",
    "            es_iters += 1\n",
    "            \n",
    "\n",
    "        if e % 50 == 0 and verbose:\n",
    "            print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(\n",
    "                e, train_loss, val_acc, best_val_acc, test_acc, best_test_acc))\n",
    "            \n",
    "        if es_iters >= es_criteria:\n",
    "            print(f\"Early stopping at {e} epochs\")\n",
    "            break\n",
    "            \n",
    "    return np.array(train_losses), np.array(val_losses)\n",
    "\n",
    "def train_step(model, graph, features, labels, train_mask, val_mask, optimizer, loss_fn):\n",
    "    \"\"\"\n",
    "    A single training step\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(graph, features)\n",
    "    loss = loss_fn(logits[train_mask], labels[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_loss = loss_fn(logits[val_mask], labels[val_mask])\n",
    "\n",
    "    return loss.item(), val_loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model_cp, graph, labels, split_idx, evaluator, best_path=None):\n",
    "#def test(model, graph, labels, split_idx, evaluator, best_path=None):\n",
    "    \"\"\"\n",
    "    Executes the OGB Evaluator to return accuracy for \n",
    "    the train, valid and test sets.  If passed in a model file path, \n",
    "    loads the parameters from that file, otherwise uses the model object\n",
    "    passed in.\n",
    "    \"\"\"\n",
    "    model = deepcopy(model_cp)\n",
    "    \n",
    "    if best_path is not None:\n",
    "        model.load_state_dict(torch.load(best_path))\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    features = graph.ndata['feat']\n",
    "    logits = model(graph, features)\n",
    "    y_pred = logits.argmax(1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': labels[split_idx['train']].reshape((-1,1)),\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': labels[split_idx['valid']].reshape((-1,1)),\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': labels[split_idx['test']].reshape((-1,1)),\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "def characterize_performance(model, graph, labels, split_idx, evaluator, best_path, verbose=False):\n",
    "    \"\"\"\n",
    "    Gets performance and compares to the Leaderboard performance for GCN.\n",
    "    Optionally (`verbose=True`) will put the performance in context with the variation\n",
    "    reported on the Leaderboard and indicate whether performance is above/below \n",
    "    1-standard deviation from the mean, as given by Leaderboard.\n",
    "    \"\"\"\n",
    "    train_acc, val_acc, test_acc = test(model, graph, labels, split_idx, evaluator, best_path)\n",
    "    print(\n",
    "        f\"Leaderboard:  Test Acc={test_acc_lb} +/- {test_acc_lb_var}, Val Acc={val_acc_lb} +/- {val_acc_lb_var}\\n\"\n",
    "        f\"Yours:        Test Acc={test_acc:.4f},            Val Acc={val_acc:.4f}\\n\"\n",
    "    )\n",
    "\n",
    "    val_lb = val_acc_lb - val_acc_lb_var\n",
    "    val_ub = val_acc_lb + val_acc_lb_var\n",
    "    \n",
    "    if verbose:\n",
    "        if not val_acc >= val_lb:\n",
    "            print(\n",
    "                f\"Validation performance is worse than LB.  Expected lower bound of {val_lb:.4f}, but got {val_acc:.4f}.\")\n",
    "        elif val_acc > val_ub:\n",
    "            print(\n",
    "                f\"Validation performance is better than LB.  Expected upper bound of {val_ub:.4f}, but got {val_acc:.4f}.\")\n",
    "        else: \n",
    "            print(\n",
    "                f\"Validation performance is in the expected range of {val_lb} - {val_ub}.\"\n",
    "            )\n",
    "    \n",
    "    test_lb = test_acc_lb - test_acc_lb_var\n",
    "    test_ub = test_acc_lb + test_acc_lb_var\n",
    "    if verbose:\n",
    "        if not test_acc >= test_lb:\n",
    "            print(\n",
    "                f\"Test performance is worse than LB.  Expected lower bound of {test_lb:.4f}, but got {test_acc:.4f}.\")\n",
    "\n",
    "        elif test_acc > test_ub:\n",
    "            print(\n",
    "                f\"Test performance is better than LB.  Expected upper bound of {test_ub:.4f}, but got {test_acc:.4f}.\")\n",
    "        else:\n",
    "            print(f\"Test performance is in the expected range of {test_lb} - {test_ub}.\")\n",
    "        \n",
    "    return val_acc, test_acc\n",
    "\n",
    "def norm_plot(curves, title):\n",
    "    \"\"\"\n",
    "    Plots normal distribution curves\n",
    "    curves: list of tuples like: (mu, sigma, label)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    for mu, sigma, label in curves:\n",
    "        x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "        ax.plot(x, stats.norm.pdf(x, mu, sigma), label=label)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "    filename = f\"norm_plot_{timestamp}.png\"\n",
    "    plt.savefig(filename)\n",
    "    \n",
    "def get_experiment_stats(model_cls, model_args, train_args, n_experiments=10):\n",
    "    \"\"\"\n",
    "    Runs an experiment multiple times to get a measure of variation\n",
    "    \"\"\"\n",
    "    results = dict()\n",
    "    for i in range(n_experiments):\n",
    "        model = model_cls(**model_args).to(train_args['device'])\n",
    "        print(f\"Starting training for experiment {i+1}\")\n",
    "        # Add experiment number to model save_path\n",
    "        train_args_cp = deepcopy(train_args)\n",
    "        save_path, file_ext = train_args_cp.pop('save_path').split('.')\n",
    "        timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "        save_path_mod = f\"{save_path}__{timestamp}_{i}.{file_ext}\"\n",
    "        \n",
    "        train_losses, val_losses = train(model=model, save_path=save_path_mod, **train_args_cp)\n",
    "        val_acc, test_acc = characterize_performance(\n",
    "            model, train_args['g_simple'], train_args['labels'], train_args['split_idx'], \n",
    "            train_args['evaluator'], save_path_mod, train_args.get('verbose', False))\n",
    "        \n",
    "        results[i] = dict(val_acc=val_acc, test_acc=test_acc)\n",
    "        print(\"Training complete\\n\")\n",
    "        \n",
    "    df_stats = pd.DataFrame(results).T.agg(['mean', 'std'])\n",
    "    return df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7360d1f5",
   "metadata": {},
   "source": [
    "Load and pre-process the arxiv graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d38df53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "dataset = DglNodePropPredDataset(name = \"ogbn-arxiv\", root = 'dataset/')\n",
    "arxiv_graph_raw = dataset.graph[0]\n",
    "#arxiv_preprocessing.load_ptv_dict('semi_tv_upto5.json')\n",
    "split_idx = dataset.get_idx_split()\n",
    "labels = dataset.labels.flatten().to(device)\n",
    "#performance of of vanilla GCN\n",
    "val_acc_lb, val_acc_lb_var, test_acc_lb, test_acc_lb_var = 0.7300, 0.0017, 0.7174, 0.0029\n",
    "evaluator = Evaluator(name = \"ogbn-arxiv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f945ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished adding 0-top vertices and 1-top vertices in main dictionary\n",
      "Finished adding 0-pseudo top vertices and 1-pseudo top vertices\n",
      "Creating vertex features for the graph now\n"
     ]
    }
   ],
   "source": [
    "file_name = \"arxiv_semi_tv_upto5.json\"\n",
    "arxiv_preprocessing_d5 = PseudoTV(file_name,graph=arxiv_graph_raw,dimension=5)\n",
    "arxiv_preprocessing.load_ptv_dict()\n",
    "arxiv_preprocessing_d5.inductive_connecting()\n",
    "arxiv_preprocessing_d5.refinement()\n",
    "print(\"Creating vertex features for the graph now\")\n",
    "arxiv_preprocessing_d5.add_vertex_features()\n",
    "arxiv_preprocessing_d5.save_dicts()\n",
    "#arxiv_preprocessing_d5.load_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f32c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43760957",
   "metadata": {},
   "source": [
    "Create the graphs with different feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "016dae9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating six different graphs..\n",
      "Graphs made\n"
     ]
    }
   ],
   "source": [
    "def add_features_to_graph(graph,features):\n",
    "    \"\"\"All the existing features and new features need to be stacked and then replaced\n",
    "    with existing features. DGL graph library doesn't support adding node features node-wise\n",
    "    Returns the graph\"\"\"\n",
    "    if not \"feat\" in graph.ndata:\n",
    "        print(\"Error! The graph should have node features called 'feat'.\")\n",
    "        return\n",
    "    else:\n",
    "        existing_node_features = graph.ndata['feat']\n",
    "    \n",
    "    if not (type(features) == dict or type(features) == collections.defaultdict):\n",
    "        print(\"Error! Features should be a dictionary\")\n",
    "        return\n",
    "    stacked_features = torch.stack(list(features.values()))\n",
    "    # Iterate through nodes and concatenate stacked_features to existing_node_features\n",
    "    concatenated_features = []\n",
    "    for node_id in graph.nodes():\n",
    "        concatenated_feature = torch.cat((existing_node_features[node_id], stacked_features[node_id]), dim=0)\n",
    "        concatenated_features.append(concatenated_feature)\n",
    "\n",
    "    concatenated_features = torch.stack(concatenated_features, dim=0)\n",
    "\n",
    "    graph.ndata['feat'] = concatenated_features\n",
    "    \n",
    "    return graph\n",
    "\n",
    "print(\"Creating six different graphs..\")\n",
    "arxiv_graph_bidirected = dgl.to_bidirected(arxiv_graph_raw)\n",
    "arxiv_graph_bidirected = dgl.add_self_loop(arxiv_graph_raw)\n",
    "#once a graph is converted, it loses node features, so they have to be added again\n",
    "arxiv_graph_bidirected.ndata['feat'] = arxiv_graph_raw.ndata['feat']\n",
    "\n",
    "arxiv_graph = dgl.add_self_loop(arxiv_graph_raw)\n",
    "arxiv_graph.ndata['feat'] = arxiv_graph_raw.ndata['feat']\n",
    "\n",
    "#add features to nodes of undirected graphs. \n",
    "arxiv_graph_bidirected_mixed_tv_d5 = add_features_to_graph(\n",
    "    arxiv_graph_bidirected,arxiv_preprocessing_d5.multi_hot_tv_dict)\n",
    "arxiv_graph_bidirected_tv_and_pi_d5 = add_features_to_graph(\n",
    "    arxiv_graph_bidirected,arxiv_preprocessing_d5.partition_indices_and_one_hot_tv)\n",
    "arxiv_graph_bidirected_refined_d5 = add_features_to_graph(\n",
    "    arxiv_graph_bidirected,arxiv_preprocessing_d5.partitioned_tv)\n",
    "\n",
    "#add features to nodes of directed graphs\n",
    "arxiv_graph_mixed_tv_d5 = add_features_to_graph(\n",
    "    arxiv_graph,arxiv_preprocessing_d5.multi_hot_tv_dict)\n",
    "arxiv_graph_tv_and_pi_d5 = add_features_to_graph(\n",
    "    arxiv_graph,arxiv_preprocessing_d5.partition_indices_and_one_hot_tv)\n",
    "arxiv_graph_refined_d5 = add_features_to_graph(\n",
    "    arxiv_graph,arxiv_preprocessing_d5.partitioned_tv)\n",
    "\n",
    "#the 'year' data is added to all graphs.\n",
    "arxiv_graph_bidirected_mixed_tv_d5.ndata['year'] = arxiv_graph_raw.ndata['year']\n",
    "arxiv_graph_bidirected_tv_and_pi_d5.ndata['year'] = arxiv_graph_raw.ndata['year']\n",
    "arxiv_graph_bidirected_refined_d5.ndata['year'] = arxiv_graph_raw.ndata['year']\n",
    "arxiv_graph_mixed_tv_d5.ndata['year'] = arxiv_graph_raw.ndata['year']\n",
    "arxiv_graph_tv_and_pi_d5.ndata['year'] = arxiv_graph_raw.ndata['year']\n",
    "arxiv_graph_refined_d5.ndata['year'] = arxiv_graph_raw.ndata['year']\n",
    "arxiv_graph.ndata['year'] = arxiv_graph_raw.ndata['year']\n",
    "arxiv_graph_bidirected.ndata['year'] = arxiv_graph_raw.ndata['year']\n",
    "print(\"Graphs made\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124d97e7",
   "metadata": {},
   "source": [
    "Create models here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362f3967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdim_inlayer(graph):\n",
    "    \"\"\"The dimensions of each node features is different, so \n",
    "    a function here is needed that can yield the dimension of the input layer\"\"\"\n",
    "    return graph.ndata['feat'].size()[1]\n",
    "\n",
    "model1_mixedtv_kwargs_d5 = dict(graph=arxiv_graph_mixed_tv_d5, \n",
    "                                input_layer=getdim_inlayer(arxiv_graph_mixed_tv_d5), \n",
    "                                hidden_layers=256, output_layer=40, \n",
    "                                num_layers=3, dropout=0.5)\n",
    "\n",
    "model1_mixedtv_d5 = GCN(**model1_mixedtv_kwargs_d5).to(device)\n",
    "\n",
    "model1_tv_and_pi_kwargs_d5 = dict(graph=arxiv_graph_tv_and_pi_d5, \n",
    "                                  input_layer=getdim_inlayer(arxiv_graph_tv_and_pi_d5), \n",
    "                                  hidden_layers=256, output_layer=40, \n",
    "                                  num_layers=3, dropout=0.5)\n",
    "\n",
    "model1_tv_and_pi_d5 = GCN(**model1_tv_and_pi_kwargs_d5).to(device)\n",
    "\n",
    "model1_refined_tv_kwargs_d5 = dict(graph=arxiv_graph_refined_d5, \n",
    "                                   input_layer=getdim_inlayer(arxiv_graph_refined_d5),\n",
    "                                   hidden_layers=256, output_layer=40, \n",
    "                                   num_layers=3, dropout=0.5)\n",
    "\n",
    "model1_refined_tv_d5 = GCN(**model1_refined_tv_kwargs_d5).to(device)\n",
    "\n",
    "model1_baseline_kwargs_d5 = dict(graph=arxiv_graph, \n",
    "                                   input_layer=getdim_inlayer(arxiv_graph),\n",
    "                                   hidden_layers=256, output_layer=40, \n",
    "                                   num_layers=3, dropout=0.5)\n",
    "\n",
    "model1_baseline_d5 = GCN(**model1_baseline_kwargs_d5).to(device)\n",
    "\n",
    "model2_mixedtv_kwargs_d5 = dict(graph=arxiv_graph_bidirected_mixed_tv_d5, \n",
    "                                input_layer=getdim_inlayer(arxiv_graph_bidirected_mixed_tv_d5), \n",
    "                                hidden_layers=256, output_layer=40, \n",
    "                                num_layers=3, dropout=0.5)\n",
    "\n",
    "model2_mixedtv_d5 = GCN(**model2_mixedtv_kwargs_d5).to(device)\n",
    "\n",
    "model2_tv_and_pi_kwargs_d5 = dict(graph=arxiv_graph_bidirected_tv_and_pi_d5, \n",
    "                                  input_layer=getdim_inlayer(arxiv_graph_bidirected_tv_and_pi_d5), \n",
    "                                  hidden_layers=256, output_layer=40, \n",
    "                                  num_layers=3, dropout=0.5)\n",
    "\n",
    "model2_tv_and_pi_d5 = GCN(**model2_tv_and_pi_kwargs_d5).to(device)\n",
    "\n",
    "model2_refined_tv_kwargs_d5 = dict(graph=arxiv_graph_bidirected_refined_d5, \n",
    "                                   input_layer=getdim_inlayer(arxiv_graph_bidirected_refined_d5),\n",
    "                                   hidden_layers=256, output_layer=40, \n",
    "                                   num_layers=3, dropout=0.5)\n",
    "\n",
    "model2_refined_tv_d5 = GCN(**model2_refined_tv_kwargs_d5).to(device)\n",
    "\n",
    "model2_baseline_kwargs_d5 = dict(graph=arxiv_graph_bidirected, \n",
    "                                   input_layer=getdim_inlayer(arxiv_graph_bidirected),\n",
    "                                   hidden_layers=256, output_layer=40, \n",
    "                                   num_layers=3, dropout=0.5)\n",
    "\n",
    "model2_baseline_d5 = GCN(**model2_baseline_kwargs_d5).to(device)\n",
    "\n",
    "\n",
    "#create different paths for the models\n",
    "model1_tv_and_pi_path_d5 = 'model1_tv_and_pi_d5'\n",
    "Path(model1_tv_and_pi_path_d5).mkdir(parents=True, exist_ok=True)\n",
    "gcn1_tv_and_pi_path_d5 = f\"{model1_tv_and_pi_path_d5}/gcn_base.model\"\n",
    "\n",
    "model1_mixed_tv_path_d5 = 'model1_mixed_tv_d5'\n",
    "Path(model1_mixed_tv_path_d5).mkdir(parents=True, exist_ok=True)\n",
    "gcn1_mixed_tv_path_d5 = f\"{model1_mixed_tv_path_d5}/gcn_base.model\"\n",
    "\n",
    "model1_refined_tv_path_d5 = 'model1_refined_tv_d5'\n",
    "Path(model1_refined_tv_path_d5).mkdir(parents=True, exist_ok=True)\n",
    "gcn1_refined_tv_path_d5 = f\"{model1_refined_tv_path_d5}/gcn_base.model\"\n",
    "\n",
    "model1_baseline_path_d5 = 'model1_baseline_d5'\n",
    "Path(model1_baseline_path_d5).mkdir(parents=True, exist_ok=True)\n",
    "gcn1_baseline_path_d5 = f\"{model1_baseline_path_d5}/gcn_base.model\"\n",
    "\n",
    "model2_tv_and_pi_path_d5 = 'model2_tv_and_pi_d5'\n",
    "Path(model2_tv_and_pi_path_d5).mkdir(parents=True, exist_ok=True)\n",
    "gcn2_tv_and_pi_path_d5 = f\"{model2_tv_and_pi_path_d5}/gcn_base.model\"\n",
    "\n",
    "model2_mixed_tv_path_d5 = 'model2_mixed_tv_d5'\n",
    "Path(model2_mixed_tv_path_d5).mkdir(parents=True, exist_ok=True)\n",
    "gcn2_mixed_tv_path_d5 = f\"{model2_mixed_tv_path_d5}/gcn_base.model\"\n",
    "\n",
    "model2_refined_tv_path_d5 = 'model2_refined_tv_d5'\n",
    "Path(model2_refined_tv_path_d5).mkdir(parents=True, exist_ok=True)\n",
    "gcn2_refined_tv_path_d5 = f\"{model2_refined_tv_path_d5}/gcn_base.model\"\n",
    "\n",
    "model2_baseline_path_d5 = 'model2_baseline_d5'\n",
    "Path(model2_baseline_path_d5).mkdir(parents=True, exist_ok=True)\n",
    "gcn2_baseline_path_d5 = f\"{model2_baseline_path_d5}/gcn_base.model\"\n",
    "\n",
    "#create training arguments to be put in the GCN module\n",
    "train_args_1_tv_and_pi_d5 = dict(\n",
    "    graph=arxiv_graph_tv_and_pi_d5, labels=labels, split_idx=split_idx, \n",
    "    epochs=500, evaluator=evaluator, device=device, \n",
    "    save_path=gcn1_tv_and_pi_path_d5, lr=5e-3, es_criteria=50,\n",
    ")\n",
    "\n",
    "train_args_1_mixed_tv_d5 = dict(\n",
    "    graph=arxiv_graph_mixed_tv_d5, labels=labels, split_idx=split_idx, \n",
    "    epochs=500, evaluator=evaluator, device=device, \n",
    "    save_path=gcn1_mixed_tv_path_d5, lr=5e-3, es_criteria=50,\n",
    ")\n",
    "\n",
    "train_args_1_refined_tv_d5 = dict(\n",
    "    graph=arxiv_graph_refined_d5, labels=labels, split_idx=split_idx, \n",
    "    epochs=500, evaluator=evaluator, device=device, \n",
    "    save_path=gcn1_refined_tv_path_d5, lr=5e-3, es_criteria=50,\n",
    ")\n",
    "\n",
    "train_args_1_baseline_d5 = dict(\n",
    "    graph=arxiv_graph, labels=labels, split_idx=split_idx, \n",
    "    epochs=500, evaluator=evaluator, device=device, \n",
    "    save_path=gcn1_baseline_path_d5, lr=5e-3, es_criteria=50,\n",
    ")\n",
    "\n",
    "train_args_2_tv_and_pi_d5 = dict(\n",
    "    graph=arxiv_graph_bidirected_tv_and_pi_d5, labels=labels, split_idx=split_idx, \n",
    "    epochs=500, evaluator=evaluator, device=device, \n",
    "    save_path=gcn2_tv_and_pi_path_d5, lr=5e-3, es_criteria=50,\n",
    ")\n",
    "\n",
    "train_args_2_mixed_tv_d5 = dict(\n",
    "    graph=arxiv_graph_bidirected_mixed_tv_d5, labels=labels, split_idx=split_idx, \n",
    "    epochs=500, evaluator=evaluator, device=device, \n",
    "    save_path=gcn2_mixed_tv_path_d5, lr=5e-3, es_criteria=50,\n",
    ")\n",
    "\n",
    "train_args_2_refined_tv_d5 = dict(\n",
    "    graph=arxiv_graph_bidirected_refined_d5, labels=labels, split_idx=split_idx, \n",
    "    epochs=500, evaluator=evaluator, device=device, \n",
    "    save_path=gcn2_refined_tv_path_d5, lr=5e-3, es_criteria=50,\n",
    ")\n",
    "\n",
    "train_args_2_baseline_d5 = dict(\n",
    "    graph=arxiv_graph_bidirected, labels=labels, split_idx=split_idx, \n",
    "    epochs=500, evaluator=evaluator, device=device, \n",
    "    save_path=gcn2_baseline_path_d5, lr=5e-3, es_criteria=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01543bd4",
   "metadata": {},
   "source": [
    "Experimental evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee2932",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training for directed graph with mixed tv features..\")\n",
    "train_losses1_mixedtv_d5, val_losses1_mixedtv_d5 = train(\n",
    "    model=model1_mixedtv_d5, verbose=True, **train_args_1_mixed_tv_d5)\n",
    "\n",
    "print(\"Training for directed graph with tv and pi..\")\n",
    "train_losses1_tv_and_pi_d5, val_losses1_tv_and_pi_d5 = train(\n",
    "    model=model1_tv_and_pi_d5, verbose=True, **train_args_1_tv_and_pi_d5)\n",
    "\n",
    "print(\"Training for directed graph with refined tv..\")\n",
    "train_losses1_refined_tv_d5, val_losses1_refined_tv_d5 = train(\n",
    "    model=model1_refined_tv_d5, verbose=True, **train_args_1_refined_tv_d5)\n",
    "\n",
    "print(\"Training for baseline directed graph..\")\n",
    "train_losses1_baseline_d5, val_losses1_baseline_d5 = train(\n",
    "    model=model1_baseline_d5, verbose=True, **train_args_1_baseline_d5)\n",
    "\n",
    "print(\"Training for undirected graph with mixed tv..\")\n",
    "train_losses2_mixedtv_d5, val_losses2_mixedtv_d5 = train(\n",
    "    model=model2_mixedtv_d5, verbose=True, **train_args_2_mixed_tv_d5)\n",
    "\n",
    "print(\"Training for undirected graph with tv and pi..\")\n",
    "train_losses2_tv_and_pi_d5, val_losses2_tv_and_pi_d5 = train(\n",
    "    model=model2_tv_and_pi_d5, verbose=True, **train_args_2_tv_and_pi_d5)\n",
    "\n",
    "print(\"Training for undirected graph with refined tv..\")\n",
    "train_losses2_refined_tv_d5, val_losses2_refined_tv_d5 = train(\n",
    "    model=model2_refined_tv_d5, verbose=True, **train_args_2_refined_tv_d5)\n",
    "\n",
    "print(\"Training for baseline undirected graph..\")\n",
    "train_losses2_baseline_d5, val_losses2_baseline_d5 = train(\n",
    "    model=model2_baseline_d5, verbose=True, **train_args_2_baseline_d5)\n",
    "\n",
    "print(\"Creating plots..\")\n",
    "plot_losses(\n",
    "    train_losses1_mixedtv_d5, val_losses1_mixedtv_d5, log=True, modelname='1_mixedtv_d5')\n",
    "plot_losses(\n",
    "    train_losses1_tv_and_pi_d5, val_losses1_tv_and_pi_d5, log=True, modelname='1_tv_and_pi_3')\n",
    "plot_losses(\n",
    "    train_losses1_refined_tv_d5, val_losses1_refined_tv_d5, log=True, modelname='1_refined_tv_d5')\n",
    "plot_losses(\n",
    "    train_losses1_baseline_d5, val_losses1_baseline_d5, log=True, modelname='1_baseline_d5')\n",
    "plot_losses(\n",
    "    train_losses2_mixedtv_d5, val_losses2_mixedtv_d5, log=True, modelname='2_mixedtv_d5')\n",
    "plot_losses(\n",
    "    train_losses2_tv_and_pi_d5, val_losses2_tv_and_pi_d5, log=True, modelname='2_tv_and_pi_3')\n",
    "plot_losses(\n",
    "    train_losses2_refined_tv_d5, val_losses2_refined_tv_d5, log=True, modelname='2_refined_tv_d5')\n",
    "plot_losses(\n",
    "    train_losses2_baseline_d5, val_losses2_baseline_d5, log=True, modelname='2_baseline_d5')\n",
    "\n",
    "print(\"Characterizing performance of the model for directed graph with tv and pi\")\n",
    "characterize_performance(\n",
    "    model1_tv_and_pi_d5, arxiv_graph_tv_and_pi_d5, labels, split_idx, evaluator, gcn1_tv_and_pi_path_d5, verbose=True)\n",
    "\n",
    "print(\"Characterizing performance of the model for directed graph with mixed tv\")\n",
    "characterize_performance(\n",
    "    model_mixedtv_d5, arxiv_graph_mixed_tv_d5, labels, split_idx, evaluator, gcn1_mixed_tv_path_d5, verbose=True)\n",
    "\n",
    "print(\"Characterizing performance of the model for directed graph with refined tv\")\n",
    "characterize_performance(\n",
    "    model1_refined_tv_d5, arxiv_graph_refined_d5, labels, split_idx, evaluator, gcn1_refined_tv_path_d5, verbose=True)\n",
    "\n",
    "print(\"Characterizing performance of the baseline model for directed graph\")\n",
    "characterize_performance(\n",
    "    model1_baseline_d5, arxiv_graph, labels, split_idx, evaluator, gcn1_baseline_path_d5, verbose=True)\n",
    "\n",
    "print(\"Characterizing performance of the model for undirected graph with tv and pi\")\n",
    "characterize_performance(\n",
    "    model2_tv_and_pi_d5, arxiv_graph_bidirected_tv_and_pi_d5, labels, split_idx, evaluator, gcn2_tv_and_pi_path_d5, verbose=True)\n",
    "\n",
    "print(\"Characterizing performance of the model for directed graph with mixed tv\")\n",
    "characterize_performance(\n",
    "    model2_mixedtv_d5, arxiv_graph_bidirected_mixed_tv_d5, labels, split_idx, evaluator, gcn2_mixed_tv_path_d5, verbose=True)\n",
    "\n",
    "print(\"Characterizing performance of the model for directed graph with refined tv\")\n",
    "characterize_performance(\n",
    "    model2_refined_tv_d5, arxiv_graph_bidirected_refined_d5, labels, split_idx, evaluator, gcn2_refined_tv_path_d5, verbose=True)\n",
    "\n",
    "print(\"Characterizing performance of the baseline model for undirected graph\")\n",
    "characterize_performance(\n",
    "    model2_baseline_d5, arxiv_graph_bidirected, labels, split_idx, evaluator, gcn2_baseline_path_d5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c097e2",
   "metadata": {},
   "source": [
    "Main training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc32d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beginning training with 10 experiments of the model for directed graph with mixed tv..\")\n",
    "df_gcn1_mixedtv_d5 = get_experiment_stats(\n",
    "    model_cls=GCN, model_args=model1_mixedtv_kwargs_d5,\n",
    "    train_args=train_args_1_mixed_tv_d5, n_experiments=10\n",
    ")\n",
    "\n",
    "print(\"Beginning training with 10 experiments of the model for directed graph with tv and pi\")\n",
    "df_gcn1_tv_and_pi_d5 = get_experiment_stats(\n",
    "    model_cls=GCN, model_args=model1_tv_and_pi_kwargs_d5,\n",
    "    train_args=train_args_1_tv_and_pi_d5, n_experiments=10\n",
    ")\n",
    "\n",
    "print(\"Beginning training with 10 experiments of the model for directed graph with refined tv\")\n",
    "df_gcn1_refined_tv_d5 = get_experiment_stats(\n",
    "    model_cls=GCN, model_args=model1_refined_tv_kwargs_d5,\n",
    "    train_args=train_args_1_refined_tv_d5, n_experiments=10\n",
    ")\n",
    "\n",
    "print(\"Beginning training with 10 experiments of the baseline model for directed graph\")\n",
    "df_gcn1_baseline_d5 = get_experiment_stats(\n",
    "    model_cls=GCN, model_args=model1_baseline_kwargs_d5,\n",
    "    train_args=train_args_1_baseline_d5, n_experiments=10\n",
    ")\n",
    "\n",
    "print(\"Beginning training with 10 experiments of the model for undirected graph with mixed tv\")\n",
    "df_gcn2_mixedtv_d5 = get_experiment_stats(\n",
    "    model_cls=GCN, model_args=model2_mixedtv_kwargs_d5,\n",
    "    train_args=train_args_2_mixed_tv_d5, n_experiments=10\n",
    ")\n",
    "\n",
    "print(\"Beginning training with 10 experiments of the model for directed graph with tv and pi\")\n",
    "df_gcn2_tv_and_pi_d5 = get_experiment_stats(\n",
    "    model_cls=GCN, model_args=model2_tv_and_pi_kwargs_d5,\n",
    "    train_args=train_args_2_tv_and_pi_d5, n_experiments=10\n",
    ")\n",
    "\n",
    "print(\"Beginning training with 10 experiments of the model for directed graph with refined tv\")\n",
    "df_gcn2_refined_tv_d5 = get_experiment_stats(\n",
    "    model_cls=GCN, model_args=model2_refined_tv_kwargs_d5,\n",
    "    train_args=train_args_2_refined_tv_d5, n_experiments=10\n",
    ")\n",
    "\n",
    "print(\"Beginning training with 10 experiments of the baseline model for directed graph\")\n",
    "df_gcn2_baseline_d5 = get_experiment_stats(\n",
    "    model_cls=GCN, model_args=model2_baseline_kwargs_d5,\n",
    "    train_args=train_args_2_baseline_d5, n_experiments=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99f621",
   "metadata": {},
   "source": [
    "Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating norm plot for directed graph with tv and pi\")\n",
    "norm_plot(\n",
    "    [\n",
    "        (test_acc_lb, test_acc_lb_var, 'Leaderboard'), \n",
    "        (df_gcn1_tv_and_pi_d5.loc['mean', 'test_acc'], df_gcn1_tv_and_pi_d5.loc['std', 'test_acc'], 'GCN'),\n",
    "    ],\n",
    "    'Test Performance'\n",
    ")\n",
    "print(\"Creating norm plot for directed graph with mixed tv\")\n",
    "norm_plot(\n",
    "    [\n",
    "        (test_acc_lb, test_acc_lb_var, 'Leaderboard'), \n",
    "        (df_gcn1_mixedtv_d5.loc['mean', 'test_acc'], df_gcn1_mixedtv_d5.loc['std', 'test_acc'], 'GCN'),\n",
    "    ],\n",
    "    'Test Performance'\n",
    ")\n",
    "print(\"Creating norm plot for directed graph with refined tv\")\n",
    "norm_plot(\n",
    "    [\n",
    "        (test_acc_lb, test_acc_lb_var, 'Leaderboard'), \n",
    "        (df_gcn1_refined_tv_d5.loc['mean', 'test_acc'], df_gcn1_refined_tv_d5.loc['std', 'test_acc'], 'GCN'),\n",
    "    ],\n",
    "    'Test Performance'\n",
    ")\n",
    "print(\"Creating norm plot for baseline directed graph\")\n",
    "norm_plot(\n",
    "    [\n",
    "        (test_acc_lb, test_acc_lb_var, 'Leaderboard'), \n",
    "        (df_gcn1_baseline_d5.loc['mean', 'test_acc'], df_gcn1_baseline_d5.loc['std', 'test_acc'], 'GCN'),\n",
    "    ],\n",
    "    'Test Performance'\n",
    ")\n",
    "print(\"Creating norm plot for undirected graph with tv and pi\")\n",
    "norm_plot(\n",
    "    [\n",
    "        (test_acc_lb, test_acc_lb_var, 'Leaderboard'), \n",
    "        (df_gcn2_tv_and_pi_d5.loc['mean', 'test_acc'], df_gcn2_tv_and_pi_d5.loc['std', 'test_acc'], 'GCN'),\n",
    "    ],\n",
    "    'Test Performance'\n",
    ")\n",
    "print(\"Creating norm plot for undirected graph with mixed tv\")\n",
    "norm_plot(\n",
    "    [\n",
    "        (test_acc_lb, test_acc_lb_var, 'Leaderboard'), \n",
    "        (df_gcn2_mixedtv_d5.loc['mean', 'test_acc'], df_gcn2_mixedtv_d5.loc['std', 'test_acc'], 'GCN'),\n",
    "    ],\n",
    "    'Test Performance'\n",
    ")\n",
    "print(\"Creating norm plot for undirected graph with refined tv\")\n",
    "norm_plot(\n",
    "    [\n",
    "        (test_acc_lb, test_acc_lb_var, 'Leaderboard'), \n",
    "        (df_gcn2_refined_tv_d5.loc['mean', 'test_acc'], df_gcn2_refined_tv_d5.loc['std', 'test_acc'], 'GCN'),\n",
    "    ],\n",
    "    'Test Performance'\n",
    ")\n",
    "print(\"Creating norm plot for baseline undirected graph\")\n",
    "norm_plot(\n",
    "    [\n",
    "        (test_acc_lb, test_acc_lb_var, 'Leaderboard'), \n",
    "        (df_gcn2_baseline_d5.loc['mean', 'test_acc'], df_gcn2_baseline_d5.loc['std', 'test_acc'], 'GCN'),\n",
    "    ],\n",
    "    'Test Performance'\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Evaluating the model for directed graph with refined tv\")\n",
    "_, p = stats.ttest_ind_from_stats(\n",
    "    test_acc_lb, test_acc_lb_var, 10,\n",
    "    df_gcn1_refined_tv_d5.loc['mean', 'test_acc'], df_gcn1_refined_tv_d5.loc['std', 'test_acc'], 10,\n",
    "    equal_var=False,\n",
    ")\n",
    "print(f\"Mean Test Accuracy Improvement: {(df_gcn1_refined_tv_d5.loc['mean', 'test_acc'] - test_acc_lb):.4f}\")\n",
    "print(f\"Probability that these are from the same performance distribution = {p*100:.0f}%\")\n",
    "\n",
    "print(\"Evaluating the model for directed graph with mixed tv\")\n",
    "_, p = stats.ttest_ind_from_stats(\n",
    "    test_acc_lb, test_acc_lb_var, 10,\n",
    "    df_gcn1_mixedtv_d5.loc['mean', 'test_acc'], df_gcn1_mixedtv_d5.loc['std', 'test_acc'], 10,\n",
    "    equal_var=False,\n",
    ")\n",
    "print(f\"Mean Test Accuracy Improvement: {(df_gcn1_mixedtv_d5.loc['mean', 'test_acc'] - test_acc_lb):.4f}\")\n",
    "print(f\"Probability that these are from the same performance distribution = {p*100:.0f}%\")\n",
    "\n",
    "print(\"Evaluating the model for directed graph with tv and pi\")\n",
    "_, p = stats.ttest_ind_from_stats(\n",
    "    test_acc_lb, test_acc_lb_var, 10,\n",
    "    df_gcn1_tv_and_pi_d5.loc['mean', 'test_acc'], df_gcn1_tv_and_pi_d5.loc['std', 'test_acc'], 10,\n",
    "    equal_var=False,\n",
    ")\n",
    "print(f\"Mean Test Accuracy Improvement: {(df_gcn1_tv_and_pi_d5.loc['mean', 'test_acc'] - test_acc_lb):.4f}\")\n",
    "print(f\"Probability that these are from the same performance distribution = {p*100:.0f}%\")\n",
    "\n",
    "print(\"Evaluating the model for baseline directed graph\")\n",
    "_, p = stats.ttest_ind_from_stats(\n",
    "    test_acc_lb, test_acc_lb_var, 10,\n",
    "    df_gcn1_baseline_d5.loc['mean', 'test_acc'], df_gcn1_baseline_d5.loc['std', 'test_acc'], 10,\n",
    "    equal_var=False,\n",
    ")\n",
    "print(f\"Mean Test Accuracy Improvement: {(df_gcn1_baseline_d5.loc['mean', 'test_acc'] - test_acc_lb):.4f}\")\n",
    "print(f\"Probability that these are from the same performance distribution = {p*100:.0f}%\")\n",
    "\n",
    "print(\"Evaluating the model for undirected graph with refined tv\")\n",
    "_, p = stats.ttest_ind_from_stats(\n",
    "    test_acc_lb, test_acc_lb_var, 10,\n",
    "    df_gcn2_refined_tv_d5.loc['mean', 'test_acc'], df_gcn2_refined_tv_d5.loc['std', 'test_acc'], 10,\n",
    "    equal_var=False,\n",
    ")\n",
    "print(f\"Mean Test Accuracy Improvement: {(df_gcn2_refined_tv_d5.loc['mean', 'test_acc'] - test_acc_lb):.4f}\")\n",
    "print(f\"Probability that these are from the same performance distribution = {p*100:.0f}%\")\n",
    "\n",
    "print(\"Evaluating the model for undirected graph with mixed tv\")\n",
    "_, p = stats.ttest_ind_from_stats(\n",
    "    test_acc_lb, test_acc_lb_var, 10,\n",
    "    df_gcn2_mixedtv_d5.loc['mean', 'test_acc'], df_gcn2_mixedtv_d5.loc['std', 'test_acc'], 10,\n",
    "    equal_var=False,\n",
    ")\n",
    "print(f\"Mean Test Accuracy Improvement: {(df_gcn2_mixedtv_d5.loc['mean', 'test_acc'] - test_acc_lb):.4f}\")\n",
    "print(f\"Probability that these are from the same performance distribution = {p*100:.0f}%\")\n",
    "\n",
    "print(\"Evaluating the model for undirected graph with tv and pi\")\n",
    "_, p = stats.ttest_ind_from_stats(\n",
    "    test_acc_lb, test_acc_lb_var, 10,\n",
    "    df_gcn2_tv_and_pi_d5.loc['mean', 'test_acc'], df_gcn2_tv_and_pi_d5.loc['std', 'test_acc'], 10,\n",
    "    equal_var=False,\n",
    ")\n",
    "print(f\"Mean Test Accuracy Improvement: {(df_gcn2_tv_and_pi_d5.loc['mean', 'test_acc'] - test_acc_lb):.4f}\")\n",
    "print(f\"Probability that these are from the same performance distribution = {p*100:.0f}%\")\n",
    "\n",
    "print(\"Evaluating the model for baseline undirected graph\")\n",
    "_, p = stats.ttest_ind_from_stats(\n",
    "    test_acc_lb, test_acc_lb_var, 10,\n",
    "    df_gcn2_baseline_d5.loc['mean', 'test_acc'], df_gcn2_baseline_d5.loc['std', 'test_acc'], 10,\n",
    "    equal_var=False,\n",
    ")\n",
    "print(f\"Mean Test Accuracy Improvement: {(df_gcn2_baseline_d5.loc['mean', 'test_acc'] - test_acc_lb):.4f}\")\n",
    "print(f\"Probability that these are from the same performance distribution = {p*100:.0f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
